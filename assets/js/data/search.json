[ { "title": "Consistent Development environments using devcontainers", "url": "/posts/dev-containers/", "categories": "blogs, tech", "tags": "devcontainers, development, environments", "date": "2025-07-06 08:00:00 +0530", "snippet": "BackgroundRecently I was working on a python project with a team of 20-30 people. From past experience of working on python, I knew that I could use python virtual environments to avoid issues in l...", "content": "BackgroundRecently I was working on a python project with a team of 20-30 people. From past experience of working on python, I knew that I could use python virtual environments to avoid issues in local development. I created few scripts to ease the setup process which would create the python virtual environment and install all the other tools required for local development. Everything looked good in theory, but soon I realized that I was spending more and more time in help debugging and fixing issues in most of people’s local environments. The immediate next step I did was to create a documentation for the steup process. It helped but not too much, I was still spending time on fixing setup related issues.One day, I was on call with another team and I noticed that they had a .devcontainer folder in 2 of their repositories. They told that its what they use to develop the project. Out of curiosity, I started reading about it and found that it could be the solution to all my setup related problems.What are DevContainers?DevContainer is a specification for using docker containers for development. Docker containers provide an isolated and consistent environment to run an application. In case of DevContainers, it allows you to define a consistent development environment across all your developers.From their website: A development container (or dev container for short) allows you to use a container as a full-featured development environment. It can be used to run an application, to separate tools, libraries, or runtimes needed for working with a codebase, and to aid in continuous integration and testing. Dev containers can be run locally or remotely, in a private or public cloud, in a variety of supporting tools and editors.How it works behind the scenes ?You start with defining few configs required to setup the development environment. These are stored in a .devcontainer folder at root of your project. devcontainer cli or any other implementation will read these config files and create a docker container as per the config/specification given. A small IDE connector service is installed onto this container which allows you to interact with devcontainer from your IDE. In case of VSCode, a VSCode server is installed on the container and your local VSCode instance connects to this server via Remote Connection feature.Prerequisites for setting up DevContainersTo begin using devcontainers you need to have a docker engine running on your machine. I prefer to use Rancher Desktop because of its permissible license and compatibility with docker cli. You can install Docker Desktop or Colima as well.The next thing which you need is a compatible IDE. I’ll be using VSCode for this example as devcontainers feature is available for free on VSCode.On VSCode you will need following extensions to interact with devcontainers: Dev Containers: ms-vscode-remote.remote-containers Remote Explorer: ms-vscode.remote-explorer Remote - SSH: ms-vscode-remote.remote-ssh Remote - Tunnels: ms-vscode.remote-serverSetting up a DevContainerCreate the DevContainer Open the project or folder where you want to setup devcontainer in VSCode. Cmd + Shift + P to open the command palette and type Dev Containers: Add Dev Container Configuration Files Now VSCode will ask if you want to use a template to create your devcontainer or create a custom one. For my need I select Python3 VSCode will ask if you want to install any features like docker-in-docker etc into your devcontainer Now VSCode asks if you want to configure additional options like dependabot. VSCode will create a .devcontainer folder and prompt you to start the devcontainer. Click on Reopen in Container to start the devcontainer.Congratulations! You have successfully created a devcontainer for your project.Customizing DevContainer for your needsAbove steps create a basic bare minimum devcontainer. You can customize it with various options. You can find the full list of supported options in devcontainer.json schema. We will be updating .devcontainer/devcontainer.json file to customize the behaviour of the container.Installing Additional featuresMy python application was using testcontainers for runing the functional tests. For this reason I wanted to install docker inside my devcontainer. There are multiple ways via which I could get docker working inside my container.I found that DevContainers allows you to install additional features like docker-in-docker or docker-outside-of-docker or Ngrok etc in a declarative fashion. You can find a list of available features here, additionally you can check GitHub for more such devcontainer features.devcontainer features are like docker images which can provide additional scripts/configs to configure your devcontainer.You need to add the feature to devcontainer.json file to install them. For example following snippet installs docker-outside-of-docker and postman:\"features\": { \"ghcr.io/devcontainers/features/docker-outside-of-docker:1\": {}, \"ghcr.io/frntn/devcontainers-features/newman:latest\": { \"version\": \"5.3.2\" }}Configuring VSCodeFor my python application I need to install few extensions and configure some settings. There are multiple ways to configure your editor. After discovering features option in devcontainer, I configured the devcontainer.json file to install the required extensions and apply the standard settings.I added following customizations section to install few extensions for my python project and configure vscode settings to use black as the code formatter:\"customizations\": { \"vscode\": { \"extensions\": [ \"ms-python.python\", \"ms-python.pylint\", \"ms-python.debugpy\", \"ms-python.vscode-pylance\", \"ms-python.black-formatter\" ], \"settings\": { \"[python]\": { \"editor.defaultFormatter\": \"ms-python.black-formatter\", \"editor.formatOnSave\": true, \"editor.codeActionsOnSave\": { \"source.organizeImports\": \"explicit\" }, }, \"black-formatter.args\": [ \"--line-length\", \"100\" ] } }}Port ForwardingI needed to connect to my application running inside the devcontainer from my local machine. For this devcontainer also provides port forwarding. You can specify which ports you want to forward in devcontainer.json file.\"forwardPorts\": [8000]Above snippet will forward port 8000 from the container to 8000 of the host. BTW, VSCode provides a temporary port forwarding feature with devcontainer. Which can come handy if you forgot to forward ports.Using docker compose files to run multiple servicesMy python project needed to connect to a Database service. To provide a consistent development environment, I again used another feature of devcontainer which is to use docker compose files to setup the devcontainer.You can define either a single docker compose file or multiple docker compose files in dockerComposeFile section of the devcontainer.json file.I created 2 files for ease of maintainance. One for the database service and another for the devcontainer.\"dockerComposeFile\": [\"docker-compose.yaml\", \"opensearch-compose.yaml\"],\"service\": \"app\", // the name of the service inside the docker compose file where you want to develop your applicationFollowing is my application docker compose file:services: app: container_name: my-project restart: unless-stopped image: mcr.microsoft.com/devcontainers/python:0-3.11 volumes: - ../..:/opt:cached # Overrides default command so things don't shut down after the process ends. command: sleep infinity depends_on: opensearch: condition: service_healthy networks: my-project-net: # All of the containers will join the same Docker bridge network aliases: - my-app environment: - PYTHONUNBUFFERED=1 ports: - \"9000:8000\" # Use \"forwardPorts\" in **devcontainer.json** to forward an app port locally. # (Adding the \"ports\" property to this file will not forward from a Codespace.)networks: my-project-net: name: my-project-netRunning custom scripts on various eventsAs you can see in above sections that my application needs OpenSearch but running OpenSearch on my local machine created many issues like it needed me to increase vm.max_map_count on the VM created by Rancher Desktop.DevContainers has an option to run scripts on various events like initialize, postCreate, postStart, etc. I used initializeCommand to solve this issue on everybody’s workstation.Also, I wanted to install the python dependencies and few other tools like iputils-ping, bind9-utils and dnsutils to on these containers. So I used postCreateCommand to install these tools.Following snippet configures the VM of Rancher Desktop to increase the vm.max_map_count before creating the devcontainer and installs the python dependencies from requirements.txt and then runs a custom script located at ./.devcontainer/post-create.sh to configure the devcontainer:\"initializeCommand\": \"~/.rd/bin/rdctl shell sudo sysctl -w vm.max_map_count=262144\",\"postCreateCommand\": \"pip install --user -r requirements.txt &amp;&amp; ./.devcontainer/post-create.sh\",Reducing setup timeEverything was going good till One day I was having a bad network and rebuilding devcontainer was taking a lot of time. So I decided to check how I can optimize the setup process and cache few things during the process. So far I was using a docker image provided by Microsoft for VS Code Dev Containers. I knew that I can use it as a base image and then install my other tools on top of it. So I created a Dockerfile and pointed my docker-compose.yaml to it.FROM mcr.microsoft.com/devcontainers/python:0-3.11-bullseyeENV PYTHONUNBUFFERED 1RUN sudo apt-get update &amp;&amp; \\ sudo apt install vim iputils-ping bind9-utils dnsutils -yThis reduced the setup time by caching the OS image I was using. But still there was room to optimize it further. So I decided to use uv for my python project which helped me in caching the dependencies it has downloaded into a local cache. You can read about caching in uv here.Troubleshootingdocker command not found Please make sure that you have a docker engine installed on your machine. Please make sure that you start the docker engine before you start the VSCode. In case you are using Rancher Desktop Make sure Prefrence -&gt; Application -&gt; Environment is set to Automatic. Make usre ~/.rd/bin is added to your PATH environment variable. Docker login issueunauthorized: your account must log in with a Personal Access Token (PAT) - learn more at docs.docker.com/go/access-tokens\" You should either logout of public DockerHub account on your local machine or you should login back to public DockerHub to fix this error. You may see this kind of error for other docker registeries as well, you can follow similar steps for those as well.Failed to create devcontainer Not enough space Make sure the VM started by your docker engine has enough space left. Make sure your host machine has enought space left. name conflicts - containers/networks/volumes etc Make sure there is no conflict between the container started by your devcontainer and other containers started via other processes Run docker ps -a or docker network ls or docker volume ls command and make sure there are no conflicts. If there are conflicts then either modify your devcontainer or remove the conflicting containers/networks/volumes. .devcontainer/post-create.sh: Permission denied Make sure ls and rdctl shell ls command gives you same output. If not then follow below steps: Open System Settings Navigate to Privacy &amp; Security Navigate to Files and Folders Make sure you have given access to Rancher Desktop Navigate to Privacy &amp; Security Navigate to Full Disk Access Make sure you have given access to Rancher Desktop Make sure .devcontainer/post-create.sh has execute permissions. Make sure Owner is set correctly. Frequent reconnects on macOS/WindowsYou may face slowness while using devcontainers on macOS/Windows. This is because of the way docker works on these platforms. These platforms do not support docker natively, so the docker engine has to create a Linux VM for running the containers.Also, the code is mounted as a volume which becomes slower due to the overheads of 2 layers of virtualization happening on these platforms.The best solution is to use cached volumes or create volumes for code inside the VM created by docker engine.You can read more on improving performance here.ConclusionIt is a great tool to simplify the development process for bigger teams. It provides a consistent environment for developers to work on.But DevContainers are not a perfect solution for everyone. For example few of the team members had old workstations which lead to slow down of development process due to the overhead added by devcontainers and docker. For them I still had to fall back to traditional way of setting up the project and I used settings.json and extensions.json file in .vscode folder to make sure that everyone has the same extensions.Few times, We had to clean up all whole docker engine to fix the name conflicts.Both Apple and Microsoft are working towards supporting docker natively. I would recommend using devcontainers if you have good workstations." }, { "title": "Let's Balance the load Part 3 - L7", "url": "/posts/creating-l7-load-balancer/", "categories": "blogs, tech", "tags": "loadbalancer, networking, l7-lb", "date": "2025-04-24 08:00:00 +0530", "snippet": "Creating a L7 Load BalancerAt L7 level we use reverse proxies to expose our services to the clients and these reverse proxies forwards the traffic to the actual servers based on various load balanc...", "content": "Creating a L7 Load BalancerAt L7 level we use reverse proxies to expose our services to the clients and these reverse proxies forwards the traffic to the actual servers based on various load balancing algorithms.Different Mecahnisms for Load Balancing in NginxNginx has 3 mechanisms for Load balancing: Round Robin Least Connections IP HashLet’s go over them one by one to see how we can set them up.Round RobinThis is the default load balancing mechanism used by Nginx. In Round Robin, requests to the application servers are distributed in a round-robin fashion.upstream myapp1 { server 192.168.1.1; server 192.168.1.2; server 192.168.1.3; server 192.168.1.4;}server { listen 80; location / { proxy_pass http://myapp1; }} Update myapp1 with your actual server address.Least ConnectionsIn order to use Least Connections, you need to specify the least_conn directive in your Nginx configuration. In Leas Connections mechanism, the next request is assigned to the server with the least number of active connections.upstream myapp1 { least_conn; server 192.168.1.1; server 192.168.1.2; server 192.168.1.3; server 192.168.1.4;}server { listen 80; location / { proxy_pass http://myapp1; }}IP HashFinally in IP Hash, a hash-function is used to determine what server should be selected for the next request typically client’s IP address is used for hashing. We can enable this by using ip_hash directive in the upstream configuration.upstream myapp1 { ip_hash; server 192.168.1.1; server 192.168.1.2; server 192.168.1.3; server 192.168.1.4;}server { listen 80; location / { proxy_pass http://myapp1; }}Configuring Nginx as a Reverse ProxyMost of the L7 load balancers are build on top of some sort of Proxies. We will use Nginx as our proxy for this example. We will use Docker to deploy our Nginx proxy and configure it for load balancing.# On 10.10.10.1docker pull nginxdocker run -it --rm -d -p 8080:80 --mount type=bind,source=\"$(pwd)\"/nginx.conf,target=/etc/nginx/conf.d/default.conf,readonly --name load-balancer nginx Here nginx.conf in the pwd is the configuration file for Nginx which has the configuration for load balancing. Please create this file with one of the configurations mentioned above.TestingTo test the load balancer you can host Nginx on those servers with custom html file.Let’s create a simple HTML file for each server. For example:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 1!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; Change the server number for every server to know which server html file is coming from.Start Nginx server on each server:# On 192.168.1.1docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server1-index.html,target=/usr/share/nginx/html/index.html,readonly --name server1 nginx# On 192.168.1.2docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server2-index.html,target=/usr/share/nginx/html/index.html,readonly --name server2 nginx# On 192.168.1.3docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server3-index.html,target=/usr/share/nginx/html/index.html,readonly --name server3 nginx# On 192.168.1.4docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server4-index.html,target=/usr/share/nginx/html/index.html,readonly --name server4 nginxNow try to load the website using ip address of the load balancer machine.curl http://10.10.10.1:8080/&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 1!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;Running the curl command again results in response from another server:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 2!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;Conclusion We implemented a basic L7 load balancer using nginx. The load balancer can distribute incoming traffic based on different algorithm. The actual web server sees that all the traffic is coming from the same IP address i.e. the IP of the load balancer (10.10.10.1). Optionaly the X-Forwarded-For header may have the IP address of the original client." }, { "title": "Let's Balance the load Part 2 - L3/L4", "url": "/posts/creating-l3-l4-load-balancer/", "categories": "blogs, tech", "tags": "loadbalancer, networking, l3/l4-lb", "date": "2025-04-23 08:00:00 +0530", "snippet": "At L3/L4 level we use virtual IP address to expose our services to the clients and these virtual IPs forwards the traffic to actual servers based on various load balancing algorithms.Creating a bas...", "content": "At L3/L4 level we use virtual IP address to expose our services to the clients and these virtual IPs forwards the traffic to actual servers based on various load balancing algorithms.Creating a basic L3/L4 Load BalancerThere are multiple ways to create a L3/L4 Load balancer for simplicity sake we will use iptables which is available on most of the unix systems.Let’s assume we have 4 servers 192.168.1.1, 192.168.1.2, 192.168.1.3 and 192.168.1.4. We have a load balancer machine with IP 10.10.10.1. Now, we want to distribute the incoming traffic on port 80 across these servers.This can be done in 4 steps per upstream server: Tell iptables that we want to apply Network Address Translation (NAT) to incoming traffic on port 80 as the packets come to our load balancer IP. iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -m statistic --mode nth --every 4 --packet 0 -j DNAT --to-destination 192.168.1.1:80 -p tcp tells iptables to apply this to tpc traffic only. -m statistic tells iptables command to use statistic module. --mode nth tells it to use nth mode which instructs iptables to apply this rule to every nth packet. --every 4 means that every fourth packet will be sent to 192.168.1.1. --packet 0 makes sure that rule should be applied to first packet of the stream and all the packets in that stream should be sent to the same destination. Tell iptables to perform reverse NAT so that packets coming from these servers can reach to the source IP correctly. iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.1 --dport 80 -j SNAT --to-source 10.10.10.1 Tell iptables to ACCEPT the packets coming to port 80 from Clients. iptables -t filter -A FORWARD -p tcp -d 192.168.1.1 --dport 80 -j ACCEPT Tell iptables to ACCEPT the packets coming to port 80 from Servers. iptables -t filter -A FORWARD -p tcp -s 192.168.1.1 --sport 80 -j ACCEPT We will repeat above steps for all the upstream servers except for the last one. We want to send all the remaining traffic to the last server. To do this we will execute the following command in step 1. Rest of the steps remain same.iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -j DNAT --to-destination 192.168.1.4:80 All these steps needs to be executed on Load Balancer Server.Put it togetherOur final iptables rules will look like this:# On 10.10.10.1# Destination 192.168.1.1iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -m statistic --mode nth --every 4 --packet 0 -j DNAT --to-destination 192.168.1.1:80iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.1 --dport 80 -j SNAT --to-source 10.10.10.1iptables -t filter -A FORWARD -p tcp -d 192.168.1.1 --dport 80 -j ACCEPTiptables -t filter -A FORWARD -p tcp -s 192.168.1.1 --sport 80 -j ACCEPT# Destination 192.168.1.2iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -m statistic --mode nth --every 3 --packet 0 -j DNAT --to-destination 192.168.1.2:80iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.2 --dport 80 -j SNAT --to-source 10.10.10.1iptables -t filter -A FORWARD -p tcp -d 192.168.1.2 --dport 80 -j ACCEPTiptables -t filter -A FORWARD -p tcp -s 192.168.1.2 --sport 80 -j ACCEPT# Destination 192.168.1.3iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -m statistic --mode nth --every 2 --packet 0 -j DNAT --to-destination 192.168.1.3:80iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.3 --dport 80 -j SNAT --to-source 10.10.10.1iptables -t filter -A FORWARD -p tcp -d 192.168.1.3 --dport 80 -j ACCEPTiptables -t filter -A FORWARD -p tcp -s 192.168.1.3 --sport 80 -j ACCEPT# Destination 192.168.1.4iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -j DNAT --to-destination 192.168.1.4:80iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.4 --dport 80 -j SNAT --to-source 10.10.10.1iptables -t filter -A FORWARD -p tcp -d 192.168.1.4 --dport 80 -j ACCEPTiptables -t filter -A FORWARD -p tcp -s 192.168.1.4 --sport 80 -j ACCEPTI’ve written a small python script which automates all the above steps and makes it easier to use. You can find the script here.TestingTo test the load balancer you can host Nginx on those servers with custom html file.Let’s create a simple HTML file for each server. For example:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 1!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; Change the server number for every server to know which server html file is coming from.Start Nginx server on each server:# On 192.168.1.1docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server1-index.html,target=/usr/share/nginx/html/index.html,readonly --name server1 nginx# On 192.168.1.2docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server2-index.html,target=/usr/share/nginx/html/index.html,readonly --name server2 nginx# On 192.168.1.3docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server3-index.html,target=/usr/share/nginx/html/index.html,readonly --name server3 nginx# On 192.168.1.4docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server4-index.html,target=/usr/share/nginx/html/index.html,readonly --name server4 nginxNow try to load the website using ip address of the load balancer machine.curl http://10.10.10.1:80/&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 1!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;Running the curl command again results in response from another server:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 2!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;Conclusion We implemented a basic L3/L4 load balancer using iptables. The load balancer distributes incoming traffic based on the round-robin algorithm. Nginx server sees that all the traffic is coming from the same IP address i.e. the IP of the load balancer (10.10.10.1)." }, { "title": "Let's Balance the load Part 1", "url": "/posts/understanding-load-balancer/", "categories": "blogs, tech", "tags": "loadbalancer, networking", "date": "2025-04-22 08:00:00 +0530", "snippet": "Need for Load BalancersIn the ever-expanding digital landscape, the number of internet users has grown from 416 million in the year 2000 to 4.70 billion in the year 2020 as per ourworldindata.org d...", "content": "Need for Load BalancersIn the ever-expanding digital landscape, the number of internet users has grown from 416 million in the year 2000 to 4.70 billion in the year 2020 as per ourworldindata.org data. In just two decades we saw an increase of internet users by 11 times. This surge in user base presents numerous challenges, one of which is the immense load placed on web servers serving these users.Servers, with their limited capacity, struggle to keep up with the growing demands. A single server can only catter to limitted amount of users only leaving many users with a poor experience. To solve this problem tech genius have applied many methods like regional servers, local websites, peer-to-peer networking, private clouds, VPNs etc.One of the most commonly used method is Load Balancing. It distributes incoming network traffic across multiple servers, thereby improving the performance and reliability of the system. You can think of it as a traffic police directing cars to different lanes so that no single lane gets congested.Imagining Load Balancers in OSI ModelLoad can be balanced at each layer of the OSI model.OSI ModelNow lets see how each we can build a load balancer at each layer:Physical LayerLet’s consider a blogging website with 2 servers and each server is capable of handling only 100 users in total. So our total capacity is 200 users. Our system will not be able to handle 201st user as we don’t have enough capacity. We can connect 100 users to the first server and the remaining 100 users to the second server directly using physical data lines. This way we have balanced the load at the physical layer.Cons: We need to create 2 physical networks. The load is getting managed manually. Lots of network cables will be required. New user addition needs manual intervention.Data link LayerNow Lets take a different example where our single server is capable of handling 100 users but our network is only capable of handling traffic from 50 users at a time. To handle such kind of scenario we can have multiple network interfaces on our single server and basically aggregate the 2 networks into one achieving link aggregation.Cons: We need more hardware. CPUs can handle limited number of Network interface cards becuase of limited PCI buses available on CPUs. Scaling becomes harder with increasing number of interfaces.Network LayerNetwork layer manages the routes and address of NIC cards. Here we can use IP address and routing tables to route the traffic to different servers based on IP address of the client machine. In IPv4 we have limited number of Public IP addresses. We use Network Address Translation (NAT) to use private IP addresses for internal networks and create routes based on our nextwork topology.Load Balancer at Network LayerCons: Routing tables can become complex with increasing number of routes. Client/Stream stickyiness is required as we are network packets need to go to same destination server in order to transmit whole data.Transport LayerTransport layer handles the protocol used to transmit data between Client and Server. Here we have port number which can be used to distribute the load among multiple services on same machine.Load Balancer at Transport LayerCons: We have limited number of ports available on a machine or network interface.Session LayerNext we have session layer which is responsible for managing the session between client and server. In TCP/IP stack, this layer would be handling the sockets. Sockets are special files in computers. They are created when a client connect to a server on a port. These files can be shared among multiple processes on the system.Cons: We can create/open limited number of sockets on a given interface.Presentation LayerFor balancing the load at this layer, We need to understand the tasks performed by Persentation layer. Persentation layer is responsible for formatting, encrypting and encoding the data before it is sent over the network. Here we can employ specialized chips or services to offload these tasks from our main server.Cons: Offloading may increase system laytency.Application LayerApplication layer is the topmost layer in OSI stack. We have protocols like HTTP, FTP, SMTP etc at this layer. We can create intelligent proxies at this layer to delegate the requests to different servers as this layer knows all the application protocols.Load Balancer at Application LayerCons: Each protocol at this layer is different and we need to create different proxies for each protocol. Protocol limitation is a big issue at this layer. for e.g. a lot of HTTP proxies have started supporting other protocols like FTP and HTTP protocol might not be able to support all features of other protocols.Types of Load BalancersThere are many ways to balance the load on the servers. But broadly they can be categorized into three types: L3/L4 - Network load balancer: This is a combination of IP and Port based load balancing. For given Source IP and Port it can route to a specific Destination IP and Port. Checkout part 2 of this blog to create a L3/L4 Load Balancer using IP Tables. L7 - Application load balancer: This is based on protocols at Application layer. Mostly it uses Proxy servers liks HTTP to route the traffic to upstream servers. It can route based on URL, cookies, headers etc as well since it has capability of extracting these details from the request. Checkout part 3 of this blog to create a L7 Load Balancer using Nginx Proxy. Global server load balancer: This type of load balancer uses a combination of other 2 type of load balancers to distribute the traffic across multiple data centers and regions across geographics.Takeaways Load balancers need stateless applications so that one request can be processed by one server and next could be by another server. Few load balancers need special hardware and could become expensive to scale. We need to find a balance between load balancing different solutions like l3/l4 and l7." }, { "title": "Solaris Package creation with relocation support", "url": "/posts/solaris-packaging/", "categories": "blogs, tech", "tags": "solaris", "date": "2013-03-13 08:00:00 +0530", "snippet": "Creating a Solaris package to ship your application to your customer or user. Making it relocatable so the user can choose to install at any location.BackgroundI was working on a application which ...", "content": "Creating a Solaris package to ship your application to your customer or user. Making it relocatable so the user can choose to install at any location.BackgroundI was working on a application which was ready to be shipped to the customer. We wanted to ship the application in such a way that its easier for the user to install. We started evaluating different ways to ship the application. After experiencing the Portable applications on Windows and the zip/dmg way of App delievery in macOS (formally OS X), the first option which came to everybody’s mind was to create a zip of our application.The advantage of zip was that the user was able to download the zip from our website and extract it on their machine to install the application.As our application development progressed we realized that zipping the application with a README file is not enough. Testers were frequently missing some or the other step and running into installation failures. We decided to add a small script which would automate the process of installing the application. It worked great till we had to work with Testers who were not familiar with installation process.Finally we decided to create a Solaris Package so the Testers and eventually our Customer/User could easily install the application and find help onliine. Solaris Packages were meeting all our requirements like it has a dependency management, it can be easily installed/uninstalled and it has a help system. Apart from this it gave us the freedom of adding various scripts to handle the installation process at different stages.Creating Solaris PackageSolaris Packages can be created in 4 simple steps listed below: Create working space where we will be creating our package. mkdir /tmp/myapp &amp;&amp; cd /tmp/myappexport MYAPP_PACKAGE_DIR=/tmp/myapp Create a pkginfo file for information about your package. Check here for more info on pkginfo file. PKG=&lt;pkg&gt;NAME=&lt;pkg&gt;VERSION=&lt;ver&gt;ARCH=&lt;arch&gt;CLASSES=noneCATEGORY=applicationVENDOR=&lt;vendor&gt;PSTAMP=`date`EMAIL=noreply@bikramjs.inBASEDIR=&lt;path&gt; Now lets create all the scripts which we will help us automate various steps during installation. # Copy all your scripts to some temp dircat /path/src/request &gt; ${MYAPP_PACKAGE_DIR}/requestcat /path/src/preinstall &gt; ${MYAPP_PACKAGE_DIR}/preinstallcat /path/src/postinstall &gt; ${MYAPP_PACKAGE_DIR}/postinstallcat /path/src/checkinstall &gt; ${MYAPP_PACKAGE_DIR}/checkinstallcat /path/src/preremove &gt; ${MYAPP_PACKAGE_DIR}/preremovecat /path/src/postremove &gt; ${MYAPP_PACKAGE_DIR}/postremovecat /path/src/copyright &gt; ${MYAPP_PACKAGE_DIR}/copyright #for copyright messagecat /path/src/depend &gt; ${MYAPP_PACKAGE_DIR}/depend #for adding package names which are required for your package in format of &lt;type&gt; &lt;pkg.abbr&gt; &lt;name&gt; There are mainly 6 scripts that you can add to your package. There are some other scripts also which are used for patching. Following is brief explaination about those scripts: request: To prompt user for input. This script will not be executed if the package is already installed with instance=overwrite in /var/sadm/install/admin/default file checkinstall: Used to check whether the package is already installed or the system meets the requirements of package or to configure the installation of your package by setting environment for other scripts. preinstall: Used to configure installation according to the environment set in checkinstall script. postinstall: Used to configure installation after the installation has happened. preremove: Used to execute some pre un-installation commands like removing the new files created by the package which were not part of the package supplied. postremove: Used to execute full cleanup and other system commands like ldconfig to remove the broken links left by the package removal. Create a Prototype file for your package to include the files which you want to ship. i pkginfoi preinstalli postinstalli checkinstalli requesti preremovei postremovei copyrighti depend i in front of all the scripts indicate that its a installation script. Please check documentation here for more info. Now we have a basic Prototype file with our scripts, copyright message and dependencies. Next we will add our application to the Prototype file: # myapp in the command below is the name of the sub folder which will contain all my files.pkgproto /path/src/dist=/myapp &gt;&gt; ${MYAPP_PACKAGE_DIR}/Prototype Create the installable package using pkgmk. Check here for more information. pkgmk -o -r / -d ${MYAPP_PACKAGE_DIR} -f ${MYAPP_PACKAGE_DIR}/Prototype Previous step creates the package in a directory which is not suitable for shiping or downloading from a website. Now we need to translate the package from directory to a data stream file. Check here for more info on pkgtrans. pkgtrans -s `pwd` ${MYAPP_PACKAGE_DIR}/myapp.pkg myapp You can use any temporary directory just replace /tmp with your choice. You can set the file permissions before creating a prototype file. You can use your custom prototype file for file mappings pointing to files in specific directory. for e.g. : i pkginfo=/home/user/mysrc/pkginfo. You can use your own custom prototype file to define the file permissions also. You can use -g flag in pkgtrans command to sign the packages to build trust among your users. Making package relocatableSo far we have see how to create a package on Solaris. Now in order to make it relocatable we need to run few extra steps during the packing process.Steps would remain same till Prototype file generation. After the Prototype file is generated we need to update our Prototype to tell it that its a relocatable package and user should be able to install the package at any location they want. Lets make the package relocateable sed -i \"s:/myapp:\\$BASEDIR/:g\" ${MYAPP_PACKAGE_DIR}/Prototype # $BASEDIR variable will make sure that package becomes relocateable Now lets create the pkg using pkgmk command with a value pointing to our package locally. pkgmk -o -r / -d ${MYAPP_PACKAGE_DIR} -f ${MYAPP_PACKAGE_DIR}/Prototype BASEDIR=/path/src/dist # BASEDIR here is a varaible Now we can use the pkgtrans command to convert it to a single file for easy distribution. pkgtrans -s `pwd` ${MYAPP_PACKAGE_DIR}/myapp.pkg myapp TestingTo test the package, you can install it on a Solaris machine and use following command to verify if the relocation support we added is working or not. Check here for all supported options in pkgadd.pkgadd –R /opt/installed_myapp myapp.pkgls -l /opt/installed_myapp/bin/myapp This is a migration from my original blog post" }, { "title": "Building gcc 4.1.2 on Solaris 10", "url": "/posts/building-gcc-on-solaris/", "categories": "blogs, tech", "tags": "solaris, gcc", "date": "2013-03-13 08:00:00 +0530", "snippet": "Compiling gcc 4.1.2 on Solaris to over come compilation and linking issues faced during porting my C++ application from RHEL.BackgroundI was working on porting an C++ Application from RHEL to Solar...", "content": "Compiling gcc 4.1.2 on Solaris to over come compilation and linking issues faced during porting my C++ application from RHEL.BackgroundI was working on porting an C++ Application from RHEL to Solaris 10. I was getting too many compile time errors when recompiling the application on Solaris. Initially I tried fixing those errors without thinking that language also has progressed and we might need a new compiler. After a few days finally I started comparing the build environment and closed on compiler version being different.Upgrading CompilerInitially like anybody else, we took the latest version of gcc available on Solaris 10 and tried compiling my code. It never worked out. I realized that language standard have progressed much further and gcc use to throw new errors which would have taken long time for me to fix.pkg install gcc-45Compiling the CompilerAfter my failed attempt to compile my code using latest gcc compiler, I started searching for pre-built packages for gcc 4.1.2 on Solaris 10 without any luck.Finally we made a decision to compile the compiler ourself. I immediately downloaded gcc 4.1.2 from gcc’s website. I saw that there is a configure script and Makefile. Looking at the documentation I ran configure script with bare minimum options followed by gmake. The compiler worked and I was able to compile my code. The testing cycle could start now.pkg install gcc-3cd gcc_source_dirmkdir objdir &amp;&amp; cd objdir../configure --enable-languages=c,c++gmakegmake installNext day, while reviewing my colleague’s code I found that there is a security issue with the compiler which we compiled. All the private methods were accessible in our library and anybody could call those methods.nm /path/to/library.a...00000000000021b0 T CLIF_arg_double00000000000021e0 T CLIF_arg_func0000000000002190 T CLIF_arg_int0000000000002140 T CLIF_arg_string00000000000021a0 T CLIF_arg_uint00000000000021c0 T CLIF_call_func0000000000001600 T CLIF_current_help0000000000001670 T CLIF_parse0000000000000fb0 T CLIF_print_arguments0000000000000e00 T CLIF_print_options0000000000001100 T CLIF_print_usage0000000000002180 T CLIF_set_double00000000000020e0 T CLIF_set_flag0000000000002160 T CLIF_set_int0000000000002120 T CLIF_set_string0000000000002170 T CLIF_set_uint0000000000002100 T CLIF_unset_flag00000000000020a0 T CLIF_version_handler...Upon debugging the library, I found that all the public/private functions which we wrote had a T denoting that the functions are public. We again started compiling gcc with different arguments. After many attempts there was a time when all of the functions even the public ones became private.Now many iterations later we were able to compile gcc for our use case using following steps: Install gcc 3.4.3pkg install gcc-3 By default Solaris 10 comes with gcc 3.4.3 and binutils 2.15. We need to compile binutils 2.18 in order gcc compiled correctly. Prepare the shell for compiling gcc:# Use ksh shellexport CONFIG_SHELL=/usr/bin/ksh# Copy/Create links to all gnu tools into one directory and remember to remove letter 'g' from the prefix of the name of executable.export PATH=/Mytools/binutils_2_18/bin:/gnutools:/sbin:/bin:/usr/bin:/usr/ccs/bin:/usr/sfw/binexport CC=\"/usr/sfw/bin/gcc -fPIC\"export CXX=\"/usr/sfw/bin/g++ -fPIC\" /Mytools/binutils_2_18 is the location where binutils 2.18 is installed. /gnutools is the location where gnu tools like gmake, gawk, gsed and gmake etc are symlinked without the g prefix. Build:cd gcc_source_dirmkdir objdir &amp;&amp; cd objdir../configure --with-gnu-as --with-as=/binutils_2_18/bin/as --with-gnu-ld --with-ld=/binutils_2_18/bin/ld --prefix=$PREFIX --enable-threads=posix --enable-checking=release --with-system-zlib --enable-shared --disable-symvers --enable-languages=c,c++gmake #referring to gnu makegmake install $PREFIX is the location where you want to install gcc. It should be writable by your user.LearningsI learned that compiling a compiler without a compiler is kind of fun. Following are the things I learned: gcc built a basic compiler before actually compiling the code. gcc configure script has so many flags which can cause security issues in the output binary like the one time when I acciedently made all the functions public. This is a migration from my original blog post" } ]
