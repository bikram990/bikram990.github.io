[ { "title": "Ship Docker images to airgapped systems using rpms", "url": "/posts/ship-docker-images-to-airgaped-using-rpms/", "categories": "blogs, tech", "tags": "docker, rpms, airgapped", "date": "2025-07-26 08:00:00 +0530", "snippet": "In previous blog we have gone through manual steps of exporting a container and importing it on an airgapped machine. In this blog we will go through how we can remove the barier of learning docker...", "content": "In previous blog we have gone through manual steps of exporting a container and importing it on an airgapped machine. In this blog we will go through how we can remove the barier of learning docker for the IT Admins.The BarierOver many years IT Admins have been installing the software via standard packages like rpm, deb, pkg etc. Now they have to learn docker for the new applications which are being shipped on docker.Earlier they used to manage the status of different services running on a machine via service or systemctl command but now they have to use docker ps to know the status or a service.They have been using journalctl to view the logs of a service, but now they have to use docker logs. Let’s see how we can help them use docker via these standard tools.Creating RPM for a docker servicePrerequisteWe will need following packages to be installed for building an RPM. podman or docker for pulling and exporting the images. rpmdevtools for setting up the RPM build area and creating spec file. rpm-build for building the RPM. dnf-plugins-core for downloading any dependent RPMs like podman or curl etc.Preparing for Building RPMBefore we start building RPM we have to know what an RPM is. RPM stands for RedHat Package Manager. RPM is a package manager which allows you to ship your software packaged as an .rpm file. RPM packages can be of 2 types Binary packages and Source packages. Binary packages are the type of packages where you package pre-built binaries and you don’t have to compile or download anything. Source packages are the type of packages where you have to package source code into the package and then compile and install it on the target machine.For this exercise we will be building a Binary package using nginx image. We are not building Source package as the application is already compiled and installed inside the container.You can use rpmdev-setuptree to setup a build area. By default it will treat your $HOME/rpmbuild as the build area. In order to change this path you have to change HOME env variable before running this command.HOME=/tmp/docker-to-rmp rpmdev-setuptreels -la /tmp/docker-to-rmp/It will create a directory structure like mentioned below:.rpmmacrosrpmbuild/├── BUILD├── RPMS├── SOURCES├── SPECS└── SRPMS BUILD folder is where you will build your code or have intermediate files. RPMS folder will contain the RPM for binary type rpms. SOURCES folder should have all the source code which you want to use for your build. SPECS folder should have your rpm spec SRPMS folder will contain the RPM, if you are building the source rpm. .rpmmacros file is macro file.The next step is to create a spec file. Spec file contains the package specification like package name, version, vendor, scripts, changelog etc.You can use rpmdev-newspec command to create a spec file.cd /tmp/docker-to-rmp/rpmbuild/SPECSrpmdev-newspec -o nginx.speccat nginx.specAbove command will create a minimal spec file which looks like below:Name: nginxVersion:Release: 1%{?dist}Summary:License:URL:Source0:BuildRequires:Requires:%description%prep%autosetup%build%configure%make_build%installrm -rf $RPM_BUILD_ROOT%make_install%files%license add-license-file-here%doc add-docs-here%changelog* Sat Jul 25 2025 root-By default rpmdev-newspec command creates the spec file suitable for Source package. We would need to modify it for our needs.Name: nginxVersion: 1.28Release: 0Summary: nginx container packaged as rpmGroup:\t\t Applications/SystemLicense:\t MITVendor:\t\t Bikramjeet SinghURL:\t\t http://blogs.bikramjs.in/Packager:\t Software Group &lt;packager@blogs.bikramjs.in&gt;BuildArch: noarchProvides: nginx%descriptionnginx container packaged as rpm%pre%install%post%files%defattr(-,root,root,-) # Tell RPM manager that the default owner of all the files is root%dir/opt/docker-rpm/nginx # Tell RPM package manger that this RPM is managing this directory and everything under it%preun%postun%changelog* Mon Jul 21 2025 Bikramjeet Singh &lt;packager@blogs.bikramjs.in&gt;- Sample changelog We can cache this spec file instead of generating it for each build.Automating image import and clean upNow the next thing is to import the docker image and make it available for use and manage the whole lifecycle of the image. RPM spec file has many script macros which can be used to automate stuff at variaous stages. Let’s outline what do we need to do and see which script makes sense for them: We need to bundle the image archive into the RPM. %install runs at RPM build time for Binary package and RPM installation for Source package. This is the perfect place to export the image. podman image pull docker.io/library/nginx:1.28podman image save docker.io/library/nginx:1.28 -o \"%{buildroot}/opt/docker-rpm/nginx/nginx-1.28.tar\" We need to import the image archive on target system where we will install the RPM. %post runs after installing the RPM. That means we will have our image archive available at /opt/docker-rpm/nginx/nginx-1.28.tar when this step is being run. podman load -i \"/opt/docker-rpm/nginx/nginx-1.28.tar\" We need to recreate the running containers when upgrading. Here first we need to identify containers which are running the old image for this we would need to split this section into %pre and %post scripts. %pre script runs before installing the package and both %pre and %post scripts get 1 argument which tells them if it is a fresh install or upgrade scenario. %preif [ $1 == 1 ];then echo \"Pre Installing nginx\"elif [ $1 == 2 ];then echo \"Pre Upgrading nginx\" OLD_IMAGE_TAG=`rpm -q --info \"nginx\" | grep \"Release : \" | cut -b 15-` echo \"docker.io/library/nginx:${OLD_IMAGE_TAG}\" &gt; \"/tmp/nginx.old_image\" podman ps -a --format=\"\\{\\{ \\.ID \\}\\} \\{\\{ .Image \\}\\}\" | grep docker.io/library/nginx | cut -b -12 &gt; \"/tmp/nginx.old_containers\" podman ps --format='\\{\\{ \\.ID \\}\\} \\{\\{ .Image \\}\\}' | grep docker.io/library/nginx | cut -b -12 &gt; \"/tmp/nginx.running_containers\"fi Please remove escape char \\ from the format. I’ve used it because the blog site generator assumes it as a variable. %postpodman load -i \"/opt/docker-rpm/nginx/nginx-1.28.tar\"if [ $1 == 1 ];then echo \"Post Installing nginx\"elif [ $1 == 2 ];then echo \"Post Upgrading nginx\" podman stop $(cat \"/tmp/nginx.running_containers\") cat /tmp/nginx.running_containers | while read CONTAINER_ID do echo \"Upgrading ${CONTAINER_ID}\" # Find the command used to create the container CREATE_COMMAND=`podman inspect ${CONTAINER_ID} | jq -M -r '.[0].Config.CreateCommand |= .[:-1] | .[0].Config.CreateCommand | join(\" \")'` # Remvoe the old container podman rm -f ${CONTAINER_ID} # Run or Create the container NEW_CONTAINER_ID=`${CREATE_COMMAND} -d docker.io/library/nginx:1.28` # Start the container in case the create_command is just creating the container podman start ${NEW_CONTAINER_ID} done fi Please note that CreateCommand section is only available in podman. You may have to tweak the command to find the container command. We need to clean up the old images on target system when user upgrades the package. %preun script runs both at after upgrade and before uninstalling. It gets an argument to differentiate between the 2 cases. if [ $1 == 1 ];then echo \"Pre Uninstall while Upgrading nginx\" OLD_IMAGE=`cat \"/tmp/nginx.old_image\"` podman image rm ${OLD_IMAGE}elif [ $1 == 0 ];then echo \"Pre Uninstalling nginx\"fi We need to clean up the image on target system when user choses to uninstall the package. podman or docker requires us to stop and remove any containers before deleting any image. So here we will have to update the %preun script and create a %postun script. %preunif [ $1 == 1 ];then echo \"Pre Uninstall while Upgrading nginx\" OLD_IMAGE=`cat \"/tmp/nginx.old_image\"` podman image rm ${OLD_IMAGE}elif [ $1 == 0 ];then echo \"Pre Uninstalling nginx\" podman ps -a --format=\"\\{\\{ \\.ID \\}\\} \\{\\{ .Image \\}\\}\" | grep docker.io/library/nginx | cut -b -12 &gt; \"/tmp/nginx.containers\" podman stop $(cat \"/tmp/nginx.containers\") podman rm -f $(cat \"/tmp/nginx.containers\")fi %postunif [ $1 == 1 ];then echo \"Post Uninstall while Upgrading nginx\"elif [ $1 == 0 ];then echo \"Post Uninstalling nginx\" podman image rm -f docker.io/library/nginx:1.28 rm -Rf /opt/docker-rpm/nginxfi Building RPMNow we our spec file is ready to be used. We need to update %_topdir in the macrofile as we changed the build area from the default build area.echo \"%_topdir /tmp/docker-to-rmp/rpmbuild/\" &gt;&gt; /tmp/docker-to-rmp/.rpmmacrosFinally we can fire the rpmbuild command to build the package.rpmbuild --load=/tmp/docker-to-rmp/.rpmmacros -bb /tmp/docker-to-rmp/rpmbuild/SPECS/nginx.spec You can define multiple macros in a single file and You can use multiple --load= flags to specify multiple macro files.Using systemctl to manage the containerWe can wrap the podman commands under a service file as well to make it easy for IT Admins to start/stop the container. To do that first we need to create a service file like below:[Unit]Description=Docker to RPMRequires=network-online.targetAfter=network-online.target[Service]Type=simpleExecStartPre=podman container create --rm --publish-all --label \"managed_by=docker-to-rpm\" --name nginx docker.io/library/nginx:1.28ExecStart=podman container start nginx# ExecReload=ExecStop=podman container stop nginx# ExecStopPost=TimeoutStartSec=120TimeoutStopSec=60# Restart behavior:# Restart the service if it fails, with a delay, to make it restart-safeRestart=on-failureRestartSec=10s[Install]WantedBy=multi-user.targetNext we would need to update our different scripts for systemctl support.%installpodman image pull docker.io/library/nginx:1.28podman image save docker.io/library/nginx:1.28 -o \"%{buildroot}/opt/docker-rpm/nginx/nginx-1.28.tar\"%preif [ $1 == 1 ];then echo \"Pre Installing nginx\"elif [ $1 == 2 ];then echo \"Pre Upgrading nginx\" if systemctl is-enabled --quiet nginx.service &amp;&amp; systemctl is-active --quiet nginx.service ; then systemctl stop nginx.service fi OLD_IMAGE_TAG=`rpm -q --info \"nginx\" | grep \"Release : \" | cut -b 15-` echo \"docker.io/library/nginx:${OLD_IMAGE_TAG}\" &gt; \"/tmp/nginx.old_image\"fi%postpodman load -i \"/opt/docker-rpm/nginx/nginx-1.28.tar\"systemctl daemon-reloadif [ $1 == 1 ];then echo \"Post Installing nginx\"elif [ $1 == 2 ];then echo \"Post Upgrading nginx\" if systemctl is-enabled --quiet nginx.service ; then systemctl start nginx.service fifi%preunif [ $1 == 1 ];then echo \"Pre Uninstall while Upgrading nginx\" OLD_IMAGE=`cat \"/tmp/nginx.old_image\"` podman image rm ${OLD_IMAGE}elif [ $1 == 0 ];then echo \"Pre Uninstalling nginx\" systemctl disable nginx.service systemctl stop nginx.servicefi%postunif [ $1 == 1 ];then echo \"Post Uninstall while Upgrading nginx\"elif [ $1 == 0 ];then echo \"Post Uninstalling nginx\" rm -Rf \"/opt/docker-rpm/nginx/\" rm -f \"/etc/systemd/system/nginx.service\" systemctl daemon-reload podman container rm -f nginx podman image rm -f docker.io/library/nginx:1.28fi Here we are assuming that there will be only 1 container for the image and the container is being managed by the systemctl service file.The service file above doesn’t direct the container logs to journalctl command. To do that we need to update our service file to attach the stdout and stderr to the container....[Service]Type=exec...ExecStart=podman container start --attach nginx... Please note that I’ve changed the service Type as well to indicate that systemctl should treat the service to be up as long as the command is running.The Service file is exposing all the ports which container exposes to random ports on the host. We can fix the ports in service file as shown below:...[Service]...ExecStartPre=podman container create --rm -p \"8080:80\" --label \"managed_by=docker-to-rpm\" --name nginx docker.io/library/nginx:1.28...Managing multi-container workloadsMost of the time we develop applications which depends on other applications like Database, Cache and so on. We can adapt above steps to orchestrate multiple containers as well. Here podman’s compose plugin can come handy to manage and connect multiple containers together.We can use multiple strategies to manage the containers via podman’s compose plugin. I’ve tried to list few of them below: We can put multiple containers in a single RPM file along with single compose file. We can have multiple app RPMs and 1 compose file RPM to orchestrate the application. We can create multiple RPMs and have with app specific compose file for each RPM. And then combine multiple compose files with -f flag.Creating Multiple RPMs with their specific compose file is the best option in my opinion as it give you the freedom to manage upgrades for the different components separately.ConclusionContainers are becoming defacto standard for shipping new software. IT Admins have to adopt new workflows to install containerized workloads on Airgapped systems. It is a tedious job to export and deploy containerized workloads. We can ease their lifes by packaging the containers as standard packages and then wrapping orchestration part as systemctl services. This provides a better control to the Software vendor in managing their software in Airgapped installations.I’ve created a python script to automate RPM creation for a given docker image. You can download it from GitHub." }, { "title": "Ship Docker images to airgapped systems", "url": "/posts/ship-docker-images-to-airgaped/", "categories": "blogs, tech", "tags": "docker, airgapped", "date": "2025-07-18 08:00:00 +0530", "snippet": "EvolutionShipping Software to a client has been a long standing issue in the Software industury. I remember when I got my first computer, We used to purchase software on CD-ROMs and install them on...", "content": "EvolutionShipping Software to a client has been a long standing issue in the Software industury. I remember when I got my first computer, We used to purchase software on CD-ROMs and install them on our computer. At that time, Companies used to bundle all the dependencies into the CD-ROMs along with the main software.As internet become popular, many companies started to allow users to download their software over internet. Over many years, the softwares started becoming more complex and bulkier. It was not easy to bundle everything into a single package. Companies started to split the software into multiple packages and prompting their users to install the dependencies separately.This resulted in another problem where companies started to request for specific versions of the dependencies and customers could not install other softwares which required different versions of the same dependency.Docker to save the dayI remember when I was working on a project where everything worked on my machine but would not work on clients machine. Later I found that I missed listing a dependency in the documentation. This reminds me of a popular meme on Docker:Docker memeAfter few months I discovered docker and proposed to use docker to ship our application. With docker we were able to ship our application along with its dependencies without conflicting with any software installed on the host machine.What is docker?Docker is a platform for developing, shiping and running applications. Docker combines functionalities provided by chroot with cgroups to provide isolation between the container and the host machine or the underlying infrastrucute. Container provides an ecosystem parallel to a VM but without the overhead of a VM.Today we have many other applications like containerd, podman, colima, rancher desktop etc competing with docker.Airgapped Systems on the riseMany companies have switched to ship their application via containers due to the isolation provided by it. This means that companies have a new challenge of managing the security of the container, fixing any CVEs or vulnerabilities in the containers or issues in software installed in the containers.Imagine if an Attacker gains access to a crucial medical system and causes harm to the patients because the Software vendor did not fix the CVEs or vulnerabilities in their container. Due to this many companies have started to airgap their crucial systems to protect their systems from malicious actors.Installing a containerized application on an Airgapped systemOn a machine with internet access Open terminal and pull the image of the application you want to install. # Pull nginx latest image from internetdocker image pull nginx:latest Export the image docker image save nginx:latest -o nginx.tar Copy to the airgapped system via scp or via pen drive scp nginx.tar crucial-system.local:/tmp On Airgapped machine Open terminal and import the container docker image load -i /tmp/nginx.tar Run the container docker run -d -p 8080:80 --name airgapped-nginx nginx On another machine on same network as Airgapped machine Open terminal and send a curl request to the crucial-system.local curl http://crucial-system.local:8080/ Response &lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; Is that enough?In order to run docker containers you need to install docker on the airgapped system. How would you install that without internet? We may need openssl to generate self signed certificates for the nginx image which we installed. It becomes challenging to install the prerequisite software required to run the application.IT Admins face many challenges like: Installing prerequisite software. Coping softwares bigger in size will need bigger pendrives or may choke the network. Installing/Upgrading the software. Configuring the software. Debugging the issues with the software Keeping the host machine up to dateLets see how we can solve few of these issues.Downloading &amp; Installing docker on airgapped machineI’ll be using RHEL 9 for this part, these commands can be modified for your linux distro. Open terminal and install dnf-plugins-core. dnf-plugins-core adds download capabilities to the dnf command. dnf install dnf-plugins-core Download docker mkdir /tmp/docker-rpms &amp;&amp; cd /tmp/docker-rpms# Add Docker Community Edition repo to the package managerdnf config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo# Download the latest version of docker along with its dependenciesdnf download docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin --resolve The --resolve flag will try to resolve all the other dependencies required by these packages which we have not specified. docker along with its required dependencies will get downloaded in the pwd ls -l -rw-r--r-- 1 root root 61961 Jul 18 01:20 container-selinux-2.237.0-1.el9_6.noarch.rpm-rw-r--r-- 1 root root 34344299 Jul 18 01:20 containerd.io-1.7.27-3.1.el9.aarch64.rpm-rw-r--r-- 1 root root 15358454 Jul 18 01:20 docker-buildx-plugin-0.23.0-1.el9.aarch64.rpm-rw-r--r-- 1 root root 19052663 Jul 18 01:20 docker-ce-28.1.1-1.el9.aarch64.rpm-rw-r--r-- 1 root root 7889252 Jul 18 01:20 docker-ce-cli-28.1.1-1.el9.aarch64.rpm-rw-r--r-- 1 root root 3043536 Jul 18 01:20 docker-ce-rootless-extras-28.1.1-1.el9.aarch64.rpm-rw-r--r-- 1 root root 13757807 Jul 18 01:20 docker-compose-plugin-2.35.1-1.el9.aarch64.rpm-rw-r--r-- 1 root root 197588 Jul 18 01:20 libselinux-utils-3.6-3.el9.aarch64.rpm-rw-r--r-- 1 root root 29183 Jul 18 01:20 'passt-selinux-0^20250217.ga1e48a0-9.el9_6.noarch.rpm'-rw-r--r-- 1 root root 251813 Jul 18 01:20 policycoreutils-3.6-2.1.el9.aarch64.rpm-rw-r--r-- 1 root root 18085 Jul 18 01:20 rpm-plugin-selinux-4.16.1.3-37.el9.aarch64.rpm-rw-r--r-- 1 root root 46473 Jul 18 01:20 selinux-policy-38.1.53-5.el9_6.noarch.rpm-rw-r--r-- 1 root root 7251875 Jul 18 01:20 selinux-policy-targeted-38.1.53-5.el9_6.noarch.rpm Now you can copy the /tmp/docker-rpms directory to a pen drive or to the airgapped machine via scp scp *.rpm crucial-system.local:/tmp/docker-rpms Please make sure /tmp/docker-rpms exists on crucial-system.local before you run the scp command. Now we can ssh to the airgapped machine and install docker on it. ssh root@crucial-system.localcd /tmp/docker-rpmsdnf install -y *.rpm We can verify if docker is installed by running docker --version command on the airgapped system. docker --version Docker version 28.1.1, build 4eba377 Docker uses a daemon to run and manage containers. We will have to enable docker service before using it. systemctl enable --now docker Reducing the size of docker image tar archivesThe default tar archives provided by docker does not provide best compression when it comes to docker images. We can leverage gzip command to compress the image further.docker image save nginx:latest | gzip &gt; nginx.tar.gz docker supports gzip, bzip2, xz or zstd archive type for loading the image. Alternatively, you can explore oci-archive format which is supported by most container runtimes.ConclusionIn this blog we explored various strategies used by software industry to ship their software. So far containers are leading the industry due to the ease of use and isolation provided by them. We saw how IT Admins are shipping docker to their airgapped systems. In next blog we will see what we can do to ease their job." }, { "title": "Consistent Development environments using devcontainers", "url": "/posts/dev-containers/", "categories": "blogs, tech", "tags": "devcontainers, development, environments", "date": "2025-07-06 08:00:00 +0530", "snippet": "BackgroundRecently I was working on a python project with a team of 20-30 people. From past experience of working on python, I knew that I could use python virtual environments to avoid issues in l...", "content": "BackgroundRecently I was working on a python project with a team of 20-30 people. From past experience of working on python, I knew that I could use python virtual environments to avoid issues in local development. I created few scripts to ease the setup process which would create the python virtual environment and install all the other tools required for local development. Everything looked good in theory, but soon I realized that I was spending more and more time in help debugging and fixing issues in most of people’s local environments. The immediate next step I did was to create a documentation for the steup process. It helped but not too much, I was still spending time on fixing setup related issues.One day, I was on call with another team and I noticed that they had a .devcontainer folder in 2 of their repositories. They told that its what they use to develop the project. Out of curiosity, I started reading about it and found that it could be the solution to all my setup related problems.What are DevContainers?DevContainer is a specification for using docker containers for development. Docker containers provide an isolated and consistent environment to run an application. In case of DevContainers, it allows you to define a consistent development environment across all your developers.From their website: A development container (or dev container for short) allows you to use a container as a full-featured development environment. It can be used to run an application, to separate tools, libraries, or runtimes needed for working with a codebase, and to aid in continuous integration and testing. Dev containers can be run locally or remotely, in a private or public cloud, in a variety of supporting tools and editors.How it works behind the scenes ?You start with defining few configs required to setup the development environment. These are stored in a .devcontainer folder at root of your project. devcontainer cli or any other implementation will read these config files and create a docker container as per the config/specification given. A small IDE connector service is installed onto this container which allows you to interact with devcontainer from your IDE. In case of VSCode, a VSCode server is installed on the container and your local VSCode instance connects to this server via Remote Connection feature.Prerequisites for setting up DevContainersTo begin using devcontainers you need to have a docker engine running on your machine. I prefer to use Rancher Desktop because of its permissible license and compatibility with docker cli. You can install Docker Desktop or Colima as well.The next thing which you need is a compatible IDE. I’ll be using VSCode for this example as devcontainers feature is available for free on VSCode.On VSCode you will need following extensions to interact with devcontainers: Dev Containers: ms-vscode-remote.remote-containers Remote Explorer: ms-vscode.remote-explorer Remote - SSH: ms-vscode-remote.remote-ssh Remote - Tunnels: ms-vscode.remote-serverSetting up a DevContainerCreate the DevContainer Open the project or folder where you want to setup devcontainer in VSCode. Cmd + Shift + P to open the command palette and type Dev Containers: Add Dev Container Configuration Files Now VSCode will ask if you want to use a template to create your devcontainer or create a custom one. For my need I select Python3 VSCode will ask if you want to install any features like docker-in-docker etc into your devcontainer Now VSCode asks if you want to configure additional options like dependabot. VSCode will create a .devcontainer folder and prompt you to start the devcontainer. Click on Reopen in Container to start the devcontainer.Congratulations! You have successfully created a devcontainer for your project.Customizing DevContainer for your needsAbove steps create a basic bare minimum devcontainer. You can customize it with various options. You can find the full list of supported options in devcontainer.json schema. We will be updating .devcontainer/devcontainer.json file to customize the behaviour of the container.Installing Additional featuresMy python application was using testcontainers for runing the functional tests. For this reason I wanted to install docker inside my devcontainer. There are multiple ways via which I could get docker working inside my container.I found that DevContainers allows you to install additional features like docker-in-docker or docker-outside-of-docker or Ngrok etc in a declarative fashion. You can find a list of available features here, additionally you can check GitHub for more such devcontainer features.devcontainer features are like docker images which can provide additional scripts/configs to configure your devcontainer.You need to add the feature to devcontainer.json file to install them. For example following snippet installs docker-outside-of-docker and postman:\"features\": { \"ghcr.io/devcontainers/features/docker-outside-of-docker:1\": {}, \"ghcr.io/frntn/devcontainers-features/newman:latest\": { \"version\": \"5.3.2\" }}Configuring VSCodeFor my python application I need to install few extensions and configure some settings. There are multiple ways to configure your editor. After discovering features option in devcontainer, I configured the devcontainer.json file to install the required extensions and apply the standard settings.I added following customizations section to install few extensions for my python project and configure vscode settings to use black as the code formatter:\"customizations\": { \"vscode\": { \"extensions\": [ \"ms-python.python\", \"ms-python.pylint\", \"ms-python.debugpy\", \"ms-python.vscode-pylance\", \"ms-python.black-formatter\" ], \"settings\": { \"[python]\": { \"editor.defaultFormatter\": \"ms-python.black-formatter\", \"editor.formatOnSave\": true, \"editor.codeActionsOnSave\": { \"source.organizeImports\": \"explicit\" }, }, \"black-formatter.args\": [ \"--line-length\", \"100\" ] } }}Port ForwardingI needed to connect to my application running inside the devcontainer from my local machine. For this devcontainer also provides port forwarding. You can specify which ports you want to forward in devcontainer.json file.\"forwardPorts\": [8000]Above snippet will forward port 8000 from the container to 8000 of the host. BTW, VSCode provides a temporary port forwarding feature with devcontainer. Which can come handy if you forgot to forward ports.Using docker compose files to run multiple servicesMy python project needed to connect to a Database service. To provide a consistent development environment, I again used another feature of devcontainer which is to use docker compose files to setup the devcontainer.You can define either a single docker compose file or multiple docker compose files in dockerComposeFile section of the devcontainer.json file.I created 2 files for ease of maintainance. One for the database service and another for the devcontainer.\"dockerComposeFile\": [\"docker-compose.yaml\", \"opensearch-compose.yaml\"],\"service\": \"app\", // the name of the service inside the docker compose file where you want to develop your applicationFollowing is my application docker compose file:services: app: container_name: my-project restart: unless-stopped image: mcr.microsoft.com/devcontainers/python:0-3.11 volumes: - ../..:/opt:cached # Overrides default command so things don't shut down after the process ends. command: sleep infinity depends_on: opensearch: condition: service_healthy networks: my-project-net: # All of the containers will join the same Docker bridge network aliases: - my-app environment: - PYTHONUNBUFFERED=1 # Use \"forwardPorts\" in **devcontainer.json** to forward an app port locally. # (Adding the \"ports\" property to this file will not forward from a Codespace.)networks: my-project-net: name: my-project-netRunning custom scripts on various eventsAs you can see in above sections that my application needs OpenSearch but running OpenSearch on my local machine created many issues like it needed me to increase vm.max_map_count on the VM created by Rancher Desktop.DevContainers has an option to run scripts on various events like initialize, postCreate, postStart, etc. I used initializeCommand to solve this issue on everybody’s workstation.Also, I wanted to install the python dependencies and few other tools like iputils-ping, bind9-utils and dnsutils to on these containers. So I used postCreateCommand to install these tools.Following snippet configures the VM of Rancher Desktop to increase the vm.max_map_count before creating the devcontainer and installs the python dependencies from requirements.txt and then runs a custom script located at ./.devcontainer/post-create.sh to configure the devcontainer:\"initializeCommand\": \"~/.rd/bin/rdctl shell sudo sysctl -w vm.max_map_count=262144\",\"postCreateCommand\": \"pip install --user -r requirements.txt &amp;&amp; ./.devcontainer/post-create.sh\",Reducing setup timeEverything was going good till One day I was having a bad network and rebuilding devcontainer was taking a lot of time. So I decided to check how I can optimize the setup process and cache few things during the process. So far I was using a docker image provided by Microsoft for VS Code Dev Containers. I knew that I can use it as a base image and then install my other tools on top of it. So I created a Dockerfile and pointed my docker-compose.yaml to it.FROM mcr.microsoft.com/devcontainers/python:0-3.11-bullseyeENV PYTHONUNBUFFERED 1RUN sudo apt-get update &amp;&amp; \\ sudo apt install vim iputils-ping bind9-utils dnsutils -yThis reduced the setup time by caching the OS image I was using. But still there was room to optimize it further. So I decided to use uv for my python project which helped me in caching the dependencies it has downloaded into a local cache. You can read about caching in uv here.Troubleshootingdocker command not found Please make sure that you have a docker engine installed on your machine. Please make sure that you start the docker engine before you start the VSCode. In case you are using Rancher Desktop Make sure Prefrence -&gt; Application -&gt; Environment is set to Automatic. Make usre ~/.rd/bin is added to your PATH environment variable. Docker login issueunauthorized: your account must log in with a Personal Access Token (PAT) - learn more at docs.docker.com/go/access-tokens\" You should either logout of public DockerHub account on your local machine or you should login back to public DockerHub to fix this error. You may see this kind of error for other docker registeries as well, you can follow similar steps for those as well.Failed to create devcontainer Not enough space Make sure the VM started by your docker engine has enough space left. Make sure your host machine has enought space left. name conflicts - containers/networks/volumes etc Make sure there is no conflict between the container started by your devcontainer and other containers started via other processes Run docker ps -a or docker network ls or docker volume ls command and make sure there are no conflicts. If there are conflicts then either modify your devcontainer or remove the conflicting containers/networks/volumes. .devcontainer/post-create.sh: Permission denied Make sure ls and rdctl shell ls command gives you same output. If not then follow below steps: Open System Settings Navigate to Privacy &amp; Security Navigate to Files and Folders Make sure you have given access to Rancher Desktop Navigate to Privacy &amp; Security Navigate to Full Disk Access Make sure you have given access to Rancher Desktop Make sure .devcontainer/post-create.sh has execute permissions. Make sure Owner is set correctly. Frequent reconnects on macOS/WindowsYou may face slowness while using devcontainers on macOS/Windows. This is because of the way docker works on these platforms. These platforms do not support docker natively, so the docker engine has to create a Linux VM for running the containers.Also, the code is mounted as a volume which becomes slower due to the overheads of 2 layers of virtualization happening on these platforms.The best solution is to use cached volumes or create volumes for code inside the VM created by docker engine.You can read more on improving performance here.It could be because of Extension host terminated unexpectedly as well. This could be caused by VSCode server and the VSCode app may experience compatibility issues. In that case you can try downgrading VSCode or the DevContainer extension. You can also try DevContainers: Rebuild Container Without Cache option it will fix any issue with VSCode server cached by docker.ConclusionIt is a great tool to simplify the development process for bigger teams. It provides a consistent environment for developers to work on.But DevContainers are not a perfect solution for everyone. For example few of the team members had old workstations which lead to slow down of development process due to the overhead added by devcontainers and docker. For them I still had to fall back to traditional way of setting up the project and I used settings.json and extensions.json file in .vscode folder to make sure that everyone has the same extensions.Few times, We had to clean up all whole docker engine to fix the name conflicts.Both Apple and Microsoft are working towards supporting docker natively. I would recommend using devcontainers if you have good workstations.The devcontainer I use for python development is available for download from GitHub." }, { "title": "Let's Balance the load Part 3 - L7", "url": "/posts/creating-l7-load-balancer/", "categories": "blogs, tech", "tags": "loadbalancer, networking, l7-lb", "date": "2025-04-24 08:00:00 +0530", "snippet": "In previous post we imagined load balancer at various layers of OSI model. In this blog we will use nginx to create L7 Load Balancer.Different Mecahnisms for Load Balancing in NginxNginx has 3 mech...", "content": "In previous post we imagined load balancer at various layers of OSI model. In this blog we will use nginx to create L7 Load Balancer.Different Mecahnisms for Load Balancing in NginxNginx has 3 mechanisms for Load balancing: Round Robin Least Connections IP HashLet’s go over them one by one to see how we can set them up.Round RobinThis is the default load balancing mechanism used by Nginx. In Round Robin, requests to the application servers are distributed in a round-robin fashion.upstream myapp1 { server 192.168.1.1; server 192.168.1.2; server 192.168.1.3; server 192.168.1.4;}server { listen 80; location / { proxy_pass http://myapp1; }} Update myapp1 with your actual server address.Least ConnectionsIn order to use Least Connections, you need to specify the least_conn directive in your Nginx configuration. In Leas Connections mechanism, the next request is assigned to the server with the least number of active connections.upstream myapp1 { least_conn; server 192.168.1.1; server 192.168.1.2; server 192.168.1.3; server 192.168.1.4;}server { listen 80; location / { proxy_pass http://myapp1; }}IP HashFinally in IP Hash, a hash-function is used to determine what server should be selected for the next request typically client’s IP address is used for hashing. We can enable this by using ip_hash directive in the upstream configuration.upstream myapp1 { ip_hash; server 192.168.1.1; server 192.168.1.2; server 192.168.1.3; server 192.168.1.4;}server { listen 80; location / { proxy_pass http://myapp1; }}Configuring Nginx as a Reverse ProxyMost of the L7 load balancers are build on top of some sort of Proxies. We will use Nginx as our proxy for this example. We will use Docker to deploy our Nginx proxy and configure it for load balancing.# On 10.10.10.1docker pull nginxdocker run -it --rm -d -p 8080:80 --mount type=bind,source=\"$(pwd)\"/nginx.conf,target=/etc/nginx/conf.d/default.conf,readonly --name load-balancer nginx Here nginx.conf in the pwd is the configuration file for Nginx which has the configuration for load balancing. Please create this file with one of the configurations mentioned above.TestingTo test the load balancer you can host Nginx on those servers with custom html file.Let’s create a simple HTML file for each server. For example:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 1!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; Change the server number for every server to know which server html file is coming from.Start Nginx server on each server:# On 192.168.1.1docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server1-index.html,target=/usr/share/nginx/html/index.html,readonly --name server1 nginx# On 192.168.1.2docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server2-index.html,target=/usr/share/nginx/html/index.html,readonly --name server2 nginx# On 192.168.1.3docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server3-index.html,target=/usr/share/nginx/html/index.html,readonly --name server3 nginx# On 192.168.1.4docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server4-index.html,target=/usr/share/nginx/html/index.html,readonly --name server4 nginxNow try to load the website using ip address of the load balancer machine.curl http://10.10.10.1:8080/&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 1!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;Running the curl command again results in response from another server:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 2!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;Conclusion We implemented a basic L7 load balancer using nginx. The load balancer can distribute incoming traffic based on different algorithm. The actual web server sees that all the traffic is coming from the same IP address i.e. the IP of the load balancer (10.10.10.1). Optionaly the X-Forwarded-For header may have the IP address of the original client." }, { "title": "Let's Balance the load Part 2 - L3/L4", "url": "/posts/creating-l3-l4-load-balancer/", "categories": "blogs, tech", "tags": "loadbalancer, networking, l3/l4-lb", "date": "2025-04-23 08:00:00 +0530", "snippet": "In previous post we imagined load balancer at various layers of OSI model. In this blog we will use iptables to create L3/L4 Load Balancer.Creating a basic L3/L4 Load BalancerThere are multiple way...", "content": "In previous post we imagined load balancer at various layers of OSI model. In this blog we will use iptables to create L3/L4 Load Balancer.Creating a basic L3/L4 Load BalancerThere are multiple ways to create a L3/L4 Load balancer for simplicity sake we will use iptables which is available on most of the unix systems.Let’s assume we have 4 servers 192.168.1.1, 192.168.1.2, 192.168.1.3 and 192.168.1.4. We have a load balancer machine with IP 10.10.10.1. Now, we want to distribute the incoming traffic on port 80 across these servers.This can be done in 4 steps per upstream server: Tell iptables that we want to apply Network Address Translation (NAT) to incoming traffic on port 80 as the packets come to our load balancer IP. iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -m statistic --mode nth --every 4 --packet 0 -j DNAT --to-destination 192.168.1.1:80 -p tcp tells iptables to apply this to tpc traffic only. -m statistic tells iptables command to use statistic module. --mode nth tells it to use nth mode which instructs iptables to apply this rule to every nth packet. --every 4 means that every fourth packet will be sent to 192.168.1.1. --packet 0 makes sure that rule should be applied to first packet of the stream and all the packets in that stream should be sent to the same destination. Tell iptables to perform reverse NAT so that packets coming from these servers can reach to the source IP correctly. iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.1 --dport 80 -j SNAT --to-source 10.10.10.1 Tell iptables to ACCEPT the packets coming to port 80 from Clients. iptables -t filter -A FORWARD -p tcp -d 192.168.1.1 --dport 80 -j ACCEPT Tell iptables to ACCEPT the packets coming to port 80 from Servers. iptables -t filter -A FORWARD -p tcp -s 192.168.1.1 --sport 80 -j ACCEPT We will repeat above steps for all the upstream servers except for the last one. We want to send all the remaining traffic to the last server. To do this we will execute the following command in step 1. Rest of the steps remain same.iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -j DNAT --to-destination 192.168.1.4:80 All these steps needs to be executed on Load Balancer Server.Put it togetherOur final iptables rules will look like this:# On 10.10.10.1# Destination 192.168.1.1iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -m statistic --mode nth --every 4 --packet 0 -j DNAT --to-destination 192.168.1.1:80iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.1 --dport 80 -j SNAT --to-source 10.10.10.1iptables -t filter -A FORWARD -p tcp -d 192.168.1.1 --dport 80 -j ACCEPTiptables -t filter -A FORWARD -p tcp -s 192.168.1.1 --sport 80 -j ACCEPT# Destination 192.168.1.2iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -m statistic --mode nth --every 3 --packet 0 -j DNAT --to-destination 192.168.1.2:80iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.2 --dport 80 -j SNAT --to-source 10.10.10.1iptables -t filter -A FORWARD -p tcp -d 192.168.1.2 --dport 80 -j ACCEPTiptables -t filter -A FORWARD -p tcp -s 192.168.1.2 --sport 80 -j ACCEPT# Destination 192.168.1.3iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -m statistic --mode nth --every 2 --packet 0 -j DNAT --to-destination 192.168.1.3:80iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.3 --dport 80 -j SNAT --to-source 10.10.10.1iptables -t filter -A FORWARD -p tcp -d 192.168.1.3 --dport 80 -j ACCEPTiptables -t filter -A FORWARD -p tcp -s 192.168.1.3 --sport 80 -j ACCEPT# Destination 192.168.1.4iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -j DNAT --to-destination 192.168.1.4:80iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.4 --dport 80 -j SNAT --to-source 10.10.10.1iptables -t filter -A FORWARD -p tcp -d 192.168.1.4 --dport 80 -j ACCEPTiptables -t filter -A FORWARD -p tcp -s 192.168.1.4 --sport 80 -j ACCEPTI’ve written a small python script which automates all the above steps and makes it easier to use. You can find the script here.TestingTo test the load balancer you can host Nginx on those servers with custom html file.Let’s create a simple HTML file for each server. For example:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 1!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; Change the server number for every server to know which server html file is coming from.Start Nginx server on each server:# On 192.168.1.1docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server1-index.html,target=/usr/share/nginx/html/index.html,readonly --name server1 nginx# On 192.168.1.2docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server2-index.html,target=/usr/share/nginx/html/index.html,readonly --name server2 nginx# On 192.168.1.3docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server3-index.html,target=/usr/share/nginx/html/index.html,readonly --name server3 nginx# On 192.168.1.4docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server4-index.html,target=/usr/share/nginx/html/index.html,readonly --name server4 nginxNow try to load the website using ip address of the load balancer machine.curl http://10.10.10.1:80/&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 1!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;Running the curl command again results in response from another server:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 2!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;Conclusion We implemented a basic L3/L4 load balancer using iptables. The load balancer distributes incoming traffic based on the round-robin algorithm. Nginx server sees that all the traffic is coming from the same IP address i.e. the IP of the load balancer (10.10.10.1).I’ve created a python script to demonstrate this load balancer which can be downloaded from GitHub." }, { "title": "Let's Balance the load Part 1", "url": "/posts/understanding-load-balancer/", "categories": "blogs, tech", "tags": "loadbalancer, networking", "date": "2025-04-22 08:00:00 +0530", "snippet": "Need for Load BalancersIn the ever-expanding digital landscape, the number of internet users has grown from 416 million in the year 2000 to 4.70 billion in the year 2020 as per ourworldindata.org d...", "content": "Need for Load BalancersIn the ever-expanding digital landscape, the number of internet users has grown from 416 million in the year 2000 to 4.70 billion in the year 2020 as per ourworldindata.org data. In just two decades we saw an increase of internet users by 11 times. This surge in user base presents numerous challenges, one of which is the immense load placed on web servers serving these users.Servers, with their limited capacity, struggle to keep up with the growing demands. A single server can only catter to limitted amount of users only leaving many users with a poor experience. To solve this problem tech genius have applied many methods like regional servers, local websites, peer-to-peer networking, private clouds, VPNs etc.One of the most commonly used method is Load Balancing. It distributes incoming network traffic across multiple servers, thereby improving the performance and reliability of the system. You can think of it as a traffic police directing cars to different lanes so that no single lane gets congested.Imagining Load Balancers in OSI ModelLoad can be balanced at each layer of the OSI model.OSI ModelNow lets see how each we can build a load balancer at each layer:Physical LayerLet’s consider a blogging website with 2 servers and each server is capable of handling only 100 users in total. So our total capacity is 200 users. Our system will not be able to handle 201st user as we don’t have enough capacity. We can connect 100 users to the first server and the remaining 100 users to the second server directly using physical data lines. This way we have balanced the load at the physical layer.Cons: We need to create 2 physical networks. The load is getting managed manually. Lots of network cables will be required. New user addition needs manual intervention.Data link LayerNow Lets take a different example where our single server is capable of handling 100 users but our network is only capable of handling traffic from 50 users at a time. To handle such kind of scenario we can have multiple network interfaces on our single server and basically aggregate the 2 networks into one achieving link aggregation.Cons: We need more hardware. CPUs can handle limited number of Network interface cards becuase of limited PCI buses available on CPUs. Scaling becomes harder with increasing number of interfaces.Network LayerNetwork layer manages the routes and address of NIC cards. Here we can use IP address and routing tables to route the traffic to different servers based on IP address of the client machine. In IPv4 we have limited number of Public IP addresses. We use Network Address Translation (NAT) to use private IP addresses for internal networks and create routes based on our nextwork topology.Load Balancer at Network LayerCons: Routing tables can become complex with increasing number of routes. Client/Stream stickyiness is required as we are network packets need to go to same destination server in order to transmit whole data.Transport LayerTransport layer handles the protocol used to transmit data between Client and Server. Here we have port number which can be used to distribute the load among multiple services on same machine.Load Balancer at Transport LayerCons: We have limited number of ports available on a machine or network interface.Session LayerNext we have session layer which is responsible for managing the session between client and server. In TCP/IP stack, this layer would be handling the sockets. Sockets are special files in computers. They are created when a client connect to a server on a port. These files can be shared among multiple processes on the system.Cons: We can create/open limited number of sockets on a given interface.Presentation LayerFor balancing the load at this layer, We need to understand the tasks performed by Persentation layer. Persentation layer is responsible for formatting, encrypting and encoding the data before it is sent over the network. Here we can employ specialized chips or services to offload these tasks from our main server.Cons: Offloading may increase system laytency.Application LayerApplication layer is the topmost layer in OSI stack. We have protocols like HTTP, FTP, SMTP etc at this layer. We can create intelligent proxies at this layer to delegate the requests to different servers as this layer knows all the application protocols.Load Balancer at Application LayerCons: Each protocol at this layer is different and we need to create different proxies for each protocol. Protocol limitation is a big issue at this layer. for e.g. a lot of HTTP proxies have started supporting other protocols like FTP and HTTP protocol might not be able to support all features of other protocols.Types of Load BalancersThere are many ways to balance the load on the servers. But broadly they can be categorized into three types: L3/L4 - Network load balancer: This is a combination of IP and Port based load balancing. For given Source IP and Port it can route to a specific Destination IP and Port. Checkout part 2 of this blog to create a L3/L4 Load Balancer using IP Tables. L7 - Application load balancer: This is based on protocols at Application layer. Mostly it uses Proxy servers liks HTTP to route the traffic to upstream servers. It can route based on URL, cookies, headers etc as well since it has capability of extracting these details from the request. Checkout part 3 of this blog to create a L7 Load Balancer using Nginx Proxy. Global server load balancer: This type of load balancer uses a combination of other 2 type of load balancers to distribute the traffic across multiple data centers and regions across geographics.Takeaways Load balancers need stateless applications so that one request can be processed by one server and next could be by another server. Few load balancers need special hardware and could become expensive to scale. We need to find a balance between load balancing different solutions like l3/l4 and l7." }, { "title": "Solaris Package creation with relocation support", "url": "/posts/solaris-packaging/", "categories": "blogs, tech", "tags": "solaris", "date": "2013-03-13 08:00:00 +0530", "snippet": "BackgroundI was working on a application which was ready to be shipped to the customer. We wanted to ship the application in such a way that its easier for the user to install. We started evaluatin...", "content": "BackgroundI was working on a application which was ready to be shipped to the customer. We wanted to ship the application in such a way that its easier for the user to install. We started evaluating different ways to ship the application. After experiencing the Portable applications on Windows and the zip/dmg way of App delievery in macOS (formally OS X), the first option which came to everybody’s mind was to create a zip of our application.The advantage of zip was that the user was able to download the zip from our website and extract it on their machine to install the application.As our application development progressed we realized that zipping the application with a README file is not enough. Testers were frequently missing some or the other step and running into installation failures. We decided to add a small script which would automate the process of installing the application. It worked great till we had to work with Testers who were not familiar with installation process.Finally we decided to create a Solaris Package so the Testers and eventually our Customer/User could easily install the application and find help onliine. Solaris Packages were meeting all our requirements like it has a dependency management, it can be easily installed/uninstalled and it has a help system. Apart from this it gave us the freedom of adding various scripts to handle the installation process at different stages.Creating Solaris PackageSolaris Packages can be created in 4 simple steps listed below: Create working space where we will be creating our package. mkdir /tmp/myapp &amp;&amp; cd /tmp/myappexport MYAPP_PACKAGE_DIR=/tmp/myapp Create a pkginfo file for information about your package. Check here for more info on pkginfo file. PKG=&lt;pkg&gt;NAME=&lt;pkg&gt;VERSION=&lt;ver&gt;ARCH=&lt;arch&gt;CLASSES=noneCATEGORY=applicationVENDOR=&lt;vendor&gt;PSTAMP=`date`EMAIL=noreply@bikramjs.inBASEDIR=&lt;path&gt; Now lets create all the scripts which we will help us automate various steps during installation. # Copy all your scripts to some temp dircat /path/src/request &gt; ${MYAPP_PACKAGE_DIR}/requestcat /path/src/preinstall &gt; ${MYAPP_PACKAGE_DIR}/preinstallcat /path/src/postinstall &gt; ${MYAPP_PACKAGE_DIR}/postinstallcat /path/src/checkinstall &gt; ${MYAPP_PACKAGE_DIR}/checkinstallcat /path/src/preremove &gt; ${MYAPP_PACKAGE_DIR}/preremovecat /path/src/postremove &gt; ${MYAPP_PACKAGE_DIR}/postremovecat /path/src/copyright &gt; ${MYAPP_PACKAGE_DIR}/copyright #for copyright messagecat /path/src/depend &gt; ${MYAPP_PACKAGE_DIR}/depend #for adding package names which are required for your package in format of &lt;type&gt; &lt;pkg.abbr&gt; &lt;name&gt; There are mainly 6 scripts that you can add to your package. There are some other scripts also which are used for patching. Following is brief explaination about those scripts: request: To prompt user for input. This script will not be executed if the package is already installed with instance=overwrite in /var/sadm/install/admin/default file checkinstall: Used to check whether the package is already installed or the system meets the requirements of package or to configure the installation of your package by setting environment for other scripts. preinstall: Used to configure installation according to the environment set in checkinstall script. postinstall: Used to configure installation after the installation has happened. preremove: Used to execute some pre un-installation commands like removing the new files created by the package which were not part of the package supplied. postremove: Used to execute full cleanup and other system commands like ldconfig to remove the broken links left by the package removal. Create a Prototype file for your package to include the files which you want to ship. i pkginfoi preinstalli postinstalli checkinstalli requesti preremovei postremovei copyrighti depend i in front of all the scripts indicate that its a installation script. Please check documentation here for more info. Now we have a basic Prototype file with our scripts, copyright message and dependencies. Next we will add our application to the Prototype file: # myapp in the command below is the name of the sub folder which will contain all my files.pkgproto /path/src/dist=/myapp &gt;&gt; ${MYAPP_PACKAGE_DIR}/Prototype Create the installable package using pkgmk. Check here for more information. pkgmk -o -r / -d ${MYAPP_PACKAGE_DIR} -f ${MYAPP_PACKAGE_DIR}/Prototype Previous step creates the package in a directory which is not suitable for shiping or downloading from a website. Now we need to translate the package from directory to a data stream file. Check here for more info on pkgtrans. pkgtrans -s `pwd` ${MYAPP_PACKAGE_DIR}/myapp.pkg myapp You can use any temporary directory just replace /tmp with your choice. You can set the file permissions before creating a prototype file. You can use your custom prototype file for file mappings pointing to files in specific directory. for e.g. : i pkginfo=/home/user/mysrc/pkginfo. You can use your own custom prototype file to define the file permissions also. You can use -g flag in pkgtrans command to sign the packages to build trust among your users. Making package relocatableSo far we have see how to create a package on Solaris. Now in order to make it relocatable we need to run few extra steps during the packing process.Steps would remain same till Prototype file generation. After the Prototype file is generated we need to update our Prototype to tell it that its a relocatable package and user should be able to install the package at any location they want. Lets make the package relocateable sed -i \"s:/myapp:\\$BASEDIR/:g\" ${MYAPP_PACKAGE_DIR}/Prototype # $BASEDIR variable will make sure that package becomes relocateable Now lets create the pkg using pkgmk command with a value pointing to our package locally. pkgmk -o -r / -d ${MYAPP_PACKAGE_DIR} -f ${MYAPP_PACKAGE_DIR}/Prototype BASEDIR=/path/src/dist # BASEDIR here is a varaible Now we can use the pkgtrans command to convert it to a single file for easy distribution. pkgtrans -s `pwd` ${MYAPP_PACKAGE_DIR}/myapp.pkg myapp TestingTo test the package, you can install it on a Solaris machine and use following command to verify if the relocation support we added is working or not. Check here for all supported options in pkgadd.pkgadd –R /opt/installed_myapp myapp.pkgls -l /opt/installed_myapp/bin/myapp This is a migration from my original blog post" }, { "title": "Building gcc 4.1.2 on Solaris 10", "url": "/posts/building-gcc-on-solaris/", "categories": "blogs, tech", "tags": "solaris, gcc", "date": "2013-03-13 08:00:00 +0530", "snippet": "BackgroundI was working on porting an C++ Application from RHEL to Solaris 10. I was getting too many compile time errors when recompiling the application on Solaris. Initially I tried fixing those...", "content": "BackgroundI was working on porting an C++ Application from RHEL to Solaris 10. I was getting too many compile time errors when recompiling the application on Solaris. Initially I tried fixing those errors without thinking that language also has progressed and we might need a new compiler. After a few days finally I started comparing the build environment and closed on compiler version being different.Upgrading CompilerInitially like anybody else, we took the latest version of gcc available on Solaris 10 and tried compiling my code. It never worked out. I realized that language standard have progressed much further and gcc use to throw new errors which would have taken long time for me to fix.pkg install gcc-45Compiling the CompilerAfter my failed attempt to compile my code using latest gcc compiler, I started searching for pre-built packages for gcc 4.1.2 on Solaris 10 without any luck.Finally we made a decision to compile the compiler ourself. I immediately downloaded gcc 4.1.2 from gcc’s website. I saw that there is a configure script and Makefile. Looking at the documentation I ran configure script with bare minimum options followed by gmake. The compiler worked and I was able to compile my code. The testing cycle could start now.pkg install gcc-3cd gcc_source_dirmkdir objdir &amp;&amp; cd objdir../configure --enable-languages=c,c++gmakegmake installNext day, while reviewing my colleague’s code I found that there is a security issue with the compiler which we compiled. All the private methods were accessible in our library and anybody could call those methods.nm /path/to/library.a...00000000000021b0 T CLIF_arg_double00000000000021e0 T CLIF_arg_func0000000000002190 T CLIF_arg_int0000000000002140 T CLIF_arg_string00000000000021a0 T CLIF_arg_uint00000000000021c0 T CLIF_call_func0000000000001600 T CLIF_current_help0000000000001670 T CLIF_parse0000000000000fb0 T CLIF_print_arguments0000000000000e00 T CLIF_print_options0000000000001100 T CLIF_print_usage0000000000002180 T CLIF_set_double00000000000020e0 T CLIF_set_flag0000000000002160 T CLIF_set_int0000000000002120 T CLIF_set_string0000000000002170 T CLIF_set_uint0000000000002100 T CLIF_unset_flag00000000000020a0 T CLIF_version_handler...Upon debugging the library, I found that all the public/private functions which we wrote had a T denoting that the functions are public. We again started compiling gcc with different arguments. After many attempts there was a time when all of the functions even the public ones became private.Now many iterations later we were able to compile gcc for our use case using following steps: Install gcc 3.4.3pkg install gcc-3 By default Solaris 10 comes with gcc 3.4.3 and binutils 2.15. We need to compile binutils 2.18 in order gcc compiled correctly. Prepare the shell for compiling gcc:# Use ksh shellexport CONFIG_SHELL=/usr/bin/ksh# Copy/Create links to all gnu tools into one directory and remember to remove letter 'g' from the prefix of the name of executable.export PATH=/Mytools/binutils_2_18/bin:/gnutools:/sbin:/bin:/usr/bin:/usr/ccs/bin:/usr/sfw/binexport CC=\"/usr/sfw/bin/gcc -fPIC\"export CXX=\"/usr/sfw/bin/g++ -fPIC\" /Mytools/binutils_2_18 is the location where binutils 2.18 is installed. /gnutools is the location where gnu tools like gmake, gawk, gsed and gmake etc are symlinked without the g prefix. Build:cd gcc_source_dirmkdir objdir &amp;&amp; cd objdir../configure --with-gnu-as --with-as=/binutils_2_18/bin/as --with-gnu-ld --with-ld=/binutils_2_18/bin/ld --prefix=$PREFIX --enable-threads=posix --enable-checking=release --with-system-zlib --enable-shared --disable-symvers --enable-languages=c,c++gmake #referring to gnu makegmake install $PREFIX is the location where you want to install gcc. It should be writable by your user.LearningsI learned that compiling a compiler without a compiler is kind of fun. Following are the things I learned: gcc built a basic compiler before actually compiling the code. gcc configure script has so many flags which can cause security issues in the output binary like the one time when I acciedently made all the functions public. This is a migration from my original blog post" } ]
