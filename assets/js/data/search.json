[ { "title": "Let's Balance the load Part 3 - L7", "url": "/posts/creating-l7-load-balancer/", "categories": "blogs, tech", "tags": "loadbalancer, networking, l7-lb", "date": "2025-04-24 08:00:00 +0530", "snippet": "Creating a L7 Load BalancerAt L7 level we use reverse proxies to expose our services to the clients and these reverse proxies forwards the traffic to the actual servers based on various load balanc...", "content": "Creating a L7 Load BalancerAt L7 level we use reverse proxies to expose our services to the clients and these reverse proxies forwards the traffic to the actual servers based on various load balancing algorithms.Different Mecahnisms for Load Balancing in NginxNginx has 3 mechanisms for Load balancing: Round Robin Least Connections IP HashLet’s go over them one by one to see how we can set them up.Round RobinThis is the default load balancing mechanism used by Nginx. In Round Robin, requests to the application servers are distributed in a round-robin fashion.upstream myapp1 { server 192.168.1.1; server 192.168.1.2; server 192.168.1.3; server 192.168.1.4;}server { listen 80; location / { proxy_pass http://myapp1; }} Update myapp1 with your actual server address.Least ConnectionsIn order to use Least Connections, you need to specify the least_conn directive in your Nginx configuration. In Leas Connections mechanism, the next request is assigned to the server with the least number of active connections.upstream myapp1 { least_conn; server 192.168.1.1; server 192.168.1.2; server 192.168.1.3; server 192.168.1.4;}server { listen 80; location / { proxy_pass http://myapp1; }}IP HashFinally in IP Hash, a hash-function is used to determine what server should be selected for the next request typically client’s IP address is used for hashing. We can enable this by using ip_hash directive in the upstream configuration.upstream myapp1 { ip_hash; server 192.168.1.1; server 192.168.1.2; server 192.168.1.3; server 192.168.1.4;}server { listen 80; location / { proxy_pass http://myapp1; }}Configuring Nginx as a Reverse ProxyMost of the L7 load balancers are build on top of some sort of Proxies. We will use Nginx as our proxy for this example. We will use Docker to deploy our Nginx proxy and configure it for load balancing.# On 10.10.10.1docker pull nginxdocker run -it --rm -d -p 8080:80 --mount type=bind,source=\"$(pwd)\"/nginx.conf,target=/etc/nginx/conf.d/default.conf,readonly --name load-balancer nginx Here nginx.conf in the pwd is the configuration file for Nginx which has the configuration for load balancing. Please create this file with one of the configurations mentioned above.TestingTo test the load balancer you can host Nginx on those servers with custom html file.Let’s create a simple HTML file for each server. For example:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 1!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; Change the server number for every server to know which server html file is coming from.Start Nginx server on each server:# On 192.168.1.1docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server1-index.html,target=/usr/share/nginx/html/index.html,readonly --name server1 nginx# On 192.168.1.2docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server2-index.html,target=/usr/share/nginx/html/index.html,readonly --name server2 nginx# On 192.168.1.3docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server3-index.html,target=/usr/share/nginx/html/index.html,readonly --name server3 nginx# On 192.168.1.4docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server4-index.html,target=/usr/share/nginx/html/index.html,readonly --name server4 nginxNow try to load the website using ip address of the load balancer machine.curl http://10.10.10.1:8080/&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 1!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;Running the curl command again results in response from another server:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 2!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;Conclusion We implemented a basic L7 load balancer using nginx. The load balancer can distribute incoming traffic based on different algorithm. The actual web server sees that all the traffic is coming from the same IP address i.e. the IP of the load balancer (10.10.10.1). Optionaly the X-Forwarded-For header may have the IP address of the original client." }, { "title": "Let's Balance the load Part 2 - L3/L4", "url": "/posts/creating-l3-l4-load-balancer/", "categories": "blogs, tech", "tags": "loadbalancer, networking, l3/l4-lb", "date": "2025-04-23 08:00:00 +0530", "snippet": "At L3/L4 level we use virtual IP address to expose our services to the clients and these virtual IPs forwards the traffic to actual servers based on various load balancing algorithms.Creating a bas...", "content": "At L3/L4 level we use virtual IP address to expose our services to the clients and these virtual IPs forwards the traffic to actual servers based on various load balancing algorithms.Creating a basic L3/L4 Load BalancerThere are multiple ways to create a L3/L4 Load balancer for simplicity sake we will use iptables which is available on most of the unix systems.Let’s assume we have 4 servers 192.168.1.1, 192.168.1.2, 192.168.1.3 and 192.168.1.4. We have a load balancer machine with IP 10.10.10.1. Now, we want to distribute the incoming traffic on port 80 across these servers.This can be done in 4 steps per upstream server: Tell iptables that we want to apply Network Address Translation (NAT) to incoming traffic on port 80 as the packets come to our load balancer IP. iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -m statistic --mode nth --every 4 --packet 0 -j DNAT --to-destination 192.168.1.1:80 -p tcp tells iptables to apply this to tpc traffic only. -m statistic tells iptables command to use statistic module. --mode nth tells it to use nth mode which instructs iptables to apply this rule to every nth packet. --every 4 means that every fourth packet will be sent to 192.168.1.1. --packet 0 makes sure that rule should be applied to first packet of the stream and all the packets in that stream should be sent to the same destination. Tell iptables to perform reverse NAT so that packets coming from these servers can reach to the source IP correctly. iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.1 --dport 80 -j SNAT --to-source 10.10.10.1 Tell iptables to ACCEPT the packets coming to port 80 from Clients. iptables -t filter -A FORWARD -p tcp -d 192.168.1.1 --dport 80 -j ACCEPT Tell iptables to ACCEPT the packets coming to port 80 from Servers. iptables -t filter -A FORWARD -p tcp -s 192.168.1.1 --sport 80 -j ACCEPT We will repeat above steps for all the upstream servers except for the last one. We want to send all the remaining traffic to the last server. To do this we will execute the following command in step 1. Rest of the steps remain same.iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -j DNAT --to-destination 192.168.1.4:80 All these steps needs to be executed on Load Balancer Server.Put it togetherOur final iptables rules will look like this:# On 10.10.10.1# Destination 192.168.1.1iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -m statistic --mode nth --every 4 --packet 0 -j DNAT --to-destination 192.168.1.1:80iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.1 --dport 80 -j SNAT --to-source 10.10.10.1iptables -t filter -A FORWARD -p tcp -d 192.168.1.1 --dport 80 -j ACCEPTiptables -t filter -A FORWARD -p tcp -s 192.168.1.1 --sport 80 -j ACCEPT# Destination 192.168.1.2iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -m statistic --mode nth --every 3 --packet 0 -j DNAT --to-destination 192.168.1.2:80iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.2 --dport 80 -j SNAT --to-source 10.10.10.1iptables -t filter -A FORWARD -p tcp -d 192.168.1.2 --dport 80 -j ACCEPTiptables -t filter -A FORWARD -p tcp -s 192.168.1.2 --sport 80 -j ACCEPT# Destination 192.168.1.3iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -m statistic --mode nth --every 2 --packet 0 -j DNAT --to-destination 192.168.1.3:80iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.3 --dport 80 -j SNAT --to-source 10.10.10.1iptables -t filter -A FORWARD -p tcp -d 192.168.1.3 --dport 80 -j ACCEPTiptables -t filter -A FORWARD -p tcp -s 192.168.1.3 --sport 80 -j ACCEPT# Destination 192.168.1.4iptables -A PREROUTING -t nat -p tcp -d 10.10.10.1 --dport 80 -j DNAT --to-destination 192.168.1.4:80iptables -A POSTROUTING -t nat -p tcp -d 192.168.1.4 --dport 80 -j SNAT --to-source 10.10.10.1iptables -t filter -A FORWARD -p tcp -d 192.168.1.4 --dport 80 -j ACCEPTiptables -t filter -A FORWARD -p tcp -s 192.168.1.4 --sport 80 -j ACCEPTI’ve written a small python script which automates all the above steps and makes it easier to use. You can find the script here.TestingTo test the load balancer you can host Nginx on those servers with custom html file.Let’s create a simple HTML file for each server. For example:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 1!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; Change the server number for every server to know which server html file is coming from.Start Nginx server on each server:# On 192.168.1.1docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server1-index.html,target=/usr/share/nginx/html/index.html,readonly --name server1 nginx# On 192.168.1.2docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server2-index.html,target=/usr/share/nginx/html/index.html,readonly --name server2 nginx# On 192.168.1.3docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server3-index.html,target=/usr/share/nginx/html/index.html,readonly --name server3 nginx# On 192.168.1.4docker pull nginxdocker run -it --rm -d -p 80:80 --mount type=bind,source=\"$(pwd)\"/server4-index.html,target=/usr/share/nginx/html/index.html,readonly --name server4 nginxNow try to load the website using ip address of the load balancer machine.curl http://10.10.10.1:80/&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 1!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;Running the curl command again results in response from another server:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to Server 2!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;Conclusion We implemented a basic L3/L4 load balancer using iptables. The load balancer distributes incoming traffic based on the round-robin algorithm. Nginx server sees that all the traffic is coming from the same IP address i.e. the IP of the load balancer (10.10.10.1)." }, { "title": "Let's Balance the load Part 1", "url": "/posts/understanding-load-balancer/", "categories": "blogs, tech", "tags": "loadbalancer, networking", "date": "2025-04-22 08:00:00 +0530", "snippet": "Need for Load BalancersIn the ever-expanding digital landscape, the number of internet users has grown from 416 million in the year 2000 to 4.70 billion in the year 2020 as per ourworldindata.org d...", "content": "Need for Load BalancersIn the ever-expanding digital landscape, the number of internet users has grown from 416 million in the year 2000 to 4.70 billion in the year 2020 as per ourworldindata.org data. In just two decades we saw an increase of internet users by 11 times. This surge in user base presents numerous challenges, one of which is the immense load placed on web servers serving these users.Servers, with their limited capacity, struggle to keep up with the growing demands. A single server can only catter to limitted amount of users only leaving many users with a poor experience. To solve this problem tech genius have applied many methods like regional servers, local websites, peer-to-peer networking, private clouds, VPNs etc.One of the most commonly used method is Load Balancing. It distributes incoming network traffic across multiple servers, thereby improving the performance and reliability of the system. You can think of it as a traffic police directing cars to different lanes so that no single lane gets congested.Imagining Load Balancers in OSI ModelLoad can be balanced at each layer of the OSI model.OSI ModelNow lets see how each we can build a load balancer at each layer:Physical LayerLet’s consider a blogging website with 2 servers and each server is capable of handling only 100 users in total. So our total capacity is 200 users. Our system will not be able to handle 201st user as we don’t have enough capacity. We can connect 100 users to the first server and the remaining 100 users to the second server directly using physical data lines. This way we have balanced the load at the physical layer.Cons: We need to create 2 physical networks. The load is getting managed manually. Lots of network cables will be required. New user addition needs manual intervention.Data link LayerNow Lets take a different example where our single server is capable of handling 100 users but our network is only capable of handling traffic from 50 users at a time. To handle such kind of scenario we can have multiple network interfaces on our single server and basically aggregate the 2 networks into one achieving link aggregation.Cons: We need more hardware. CPUs can handle limited number of Network interface cards becuase of limited PCI buses available on CPUs. Scaling becomes harder with increasing number of interfaces.Network LayerNetwork layer manages the routes and address of NIC cards. Here we can use IP address and routing tables to route the traffic to different servers based on IP address of the client machine. In IPv4 we have limited number of Public IP addresses. We use Network Address Translation (NAT) to use private IP addresses for internal networks and create routes based on our nextwork topology.Load Balancer at Network LayerCons: Routing tables can become complex with increasing number of routes. Client/Stream stickyiness is required as we are network packets need to go to same destination server in order to transmit whole data.Transport LayerTransport layer handles the protocol used to transmit data between Client and Server. Here we have port number which can be used to distribute the load among multiple services on same machine.Load Balancer at Transport LayerCons: We have limited number of ports available on a machine or network interface.Session LayerNext we have session layer which is responsible for managing the session between client and server. In TCP/IP stack, this layer would be handling the sockets. Sockets are special files in computers. They are created when a client connect to a server on a port. These files can be shared among multiple processes on the system.Cons: We can create/open limited number of sockets on a given interface.Presentation LayerFor balancing the load at this layer, We need to understand the tasks performed by Persentation layer. Persentation layer is responsible for formatting, encrypting and encoding the data before it is sent over the network. Here we can employ specialized chips or services to offload these tasks from our main server.Cons: Offloading may increase system laytency.Application LayerApplication layer is the topmost layer in OSI stack. We have protocols like HTTP, FTP, SMTP etc at this layer. We can create intelligent proxies at this layer to delegate the requests to different servers as this layer knows all the application protocols.Load Balancer at Application LayerCons: Each protocol at this layer is different and we need to create different proxies for each protocol. Protocol limitation is a big issue at this layer. for e.g. a lot of HTTP proxies have started supporting other protocols like FTP and HTTP protocol might not be able to support all features of other protocols.Types of Load BalancersThere are many ways to balance the load on the servers. But broadly they can be categorized into three types: L3/L4 - Network load balancer: This is a combination of IP and Port based load balancing. For given Source IP and Port it can route to a specific Destination IP and Port. Checkout part 2 of this blog to create a L3/L4 Load Balancer using IP Tables. L7 - Application load balancer: This is based on protocols at Application layer. Mostly it uses Proxy servers liks HTTP to route the traffic to upstream servers. It can route based on URL, cookies, headers etc as well since it has capability of extracting these details from the request. Checkout part 3 of this blog to create a L7 Load Balancer using Nginx Proxy. Global server load balancer: This type of load balancer uses a combination of other 2 type of load balancers to distribute the traffic across multiple data centers and regions across geographics.Takeaways Load balancers need stateless applications so that one request can be processed by one server and next could be by another server. Few load balancers need special hardware and could become expensive to scale. We need to find a balance between load balancing different solutions like l3/l4 and l7." }, { "title": "Solaris Package creation with relocation support", "url": "/posts/solaris-packaging/", "categories": "blogs, tech", "tags": "solaris", "date": "2013-03-13 08:00:00 +0530", "snippet": "Creating a Solaris package to ship your application to your customer or user. Making it relocatable so the user can choose to install at any location.BackgroundI was working on a application which ...", "content": "Creating a Solaris package to ship your application to your customer or user. Making it relocatable so the user can choose to install at any location.BackgroundI was working on a application which was ready to be shipped to the customer. We wanted to ship the application in such a way that its easier for the user to install. We started evaluating different ways to ship the application. After experiencing the Portable applications on Windows and the zip/dmg way of App delievery in macOS (formally OS X), the first option which came to everybody’s mind was to create a zip of our application.The advantage of zip was that the user was able to download the zip from our website and extract it on their machine to install the application.As our application development progressed we realized that zipping the application with a README file is not enough. Testers were frequently missing some or the other step and running into installation failures. We decided to add a small script which would automate the process of installing the application. It worked great till we had to work with Testers who were not familiar with installation process.Finally we decided to create a Solaris Package so the Testers and eventually our Customer/User could easily install the application and find help onliine. Solaris Packages were meeting all our requirements like it has a dependency management, it can be easily installed/uninstalled and it has a help system. Apart from this it gave us the freedom of adding various scripts to handle the installation process at different stages.Creating Solaris PackageSolaris Packages can be created in 4 simple steps listed below: Create working space where we will be creating our package. mkdir /tmp/myapp &amp;&amp; cd /tmp/myappexport MYAPP_PACKAGE_DIR=/tmp/myapp Create a pkginfo file for information about your package. Check here for more info on pkginfo file. PKG=&lt;pkg&gt;NAME=&lt;pkg&gt;VERSION=&lt;ver&gt;ARCH=&lt;arch&gt;CLASSES=noneCATEGORY=applicationVENDOR=&lt;vendor&gt;PSTAMP=`date`EMAIL=noreply@bikramjs.inBASEDIR=&lt;path&gt; Now lets create all the scripts which we will help us automate various steps during installation. # Copy all your scripts to some temp dircat /path/src/request &gt; ${MYAPP_PACKAGE_DIR}/requestcat /path/src/preinstall &gt; ${MYAPP_PACKAGE_DIR}/preinstallcat /path/src/postinstall &gt; ${MYAPP_PACKAGE_DIR}/postinstallcat /path/src/checkinstall &gt; ${MYAPP_PACKAGE_DIR}/checkinstallcat /path/src/preremove &gt; ${MYAPP_PACKAGE_DIR}/preremovecat /path/src/postremove &gt; ${MYAPP_PACKAGE_DIR}/postremovecat /path/src/copyright &gt; ${MYAPP_PACKAGE_DIR}/copyright #for copyright messagecat /path/src/depend &gt; ${MYAPP_PACKAGE_DIR}/depend #for adding package names which are required for your package in format of &lt;type&gt; &lt;pkg.abbr&gt; &lt;name&gt; There are mainly 6 scripts that you can add to your package. There are some other scripts also which are used for patching. Following is brief explaination about those scripts: request: To prompt user for input. This script will not be executed if the package is already installed with instance=overwrite in /var/sadm/install/admin/default file checkinstall: Used to check whether the package is already installed or the system meets the requirements of package or to configure the installation of your package by setting environment for other scripts. preinstall: Used to configure installation according to the environment set in checkinstall script. postinstall: Used to configure installation after the installation has happened. preremove: Used to execute some pre un-installation commands like removing the new files created by the package which were not part of the package supplied. postremove: Used to execute full cleanup and other system commands like ldconfig to remove the broken links left by the package removal. Create a Prototype file for your package to include the files which you want to ship. i pkginfoi preinstalli postinstalli checkinstalli requesti preremovei postremovei copyrighti depend i in front of all the scripts indicate that its a installation script. Please check documentation here for more info. Now we have a basic Prototype file with our scripts, copyright message and dependencies. Next we will add our application to the Prototype file: # myapp in the command below is the name of the sub folder which will contain all my files.pkgproto /path/src/dist=/myapp &gt;&gt; ${MYAPP_PACKAGE_DIR}/Prototype Create the installable package using pkgmk. Check here for more information. pkgmk -o -r / -d ${MYAPP_PACKAGE_DIR} -f ${MYAPP_PACKAGE_DIR}/Prototype Previous step creates the package in a directory which is not suitable for shiping or downloading from a website. Now we need to translate the package from directory to a data stream file. Check here for more info on pkgtrans. pkgtrans -s `pwd` ${MYAPP_PACKAGE_DIR}/myapp.pkg myapp You can use any temporary directory just replace /tmp with your choice. You can set the file permissions before creating a prototype file. You can use your custom prototype file for file mappings pointing to files in specific directory. for e.g. : i pkginfo=/home/user/mysrc/pkginfo. You can use your own custom prototype file to define the file permissions also. You can use -g flag in pkgtrans command to sign the packages to build trust among your users. Making package relocatableSo far we have see how to create a package on Solaris. Now in order to make it relocatable we need to run few extra steps during the packing process.Steps would remain same till Prototype file generation. After the Prototype file is generated we need to update our Prototype to tell it that its a relocatable package and user should be able to install the package at any location they want. Lets make the package relocateable sed -i \"s:/myapp:\\$BASEDIR/:g\" ${MYAPP_PACKAGE_DIR}/Prototype # $BASEDIR variable will make sure that package becomes relocateable Now lets create the pkg using pkgmk command with a value pointing to our package locally. pkgmk -o -r / -d ${MYAPP_PACKAGE_DIR} -f ${MYAPP_PACKAGE_DIR}/Prototype BASEDIR=/path/src/dist # BASEDIR here is a varaible Now we can use the pkgtrans command to convert it to a single file for easy distribution. pkgtrans -s `pwd` ${MYAPP_PACKAGE_DIR}/myapp.pkg myapp TestingTo test the package, you can install it on a Solaris machine and use following command to verify if the relocation support we added is working or not. Check here for all supported options in pkgadd.pkgadd –R /opt/installed_myapp myapp.pkgls -l /opt/installed_myapp/bin/myapp" }, { "title": "Building gcc 4.1.2 on Solaris 10", "url": "/posts/building-gcc-on-solaris/", "categories": "blogs, tech", "tags": "solaris, gcc", "date": "2013-03-13 08:00:00 +0530", "snippet": "Compiling gcc 4.1.2 on Solaris to over come compilation and linking issues faced during porting my C++ application from RHEL.BackgroundI was working on porting an C++ Application from RHEL to Solar...", "content": "Compiling gcc 4.1.2 on Solaris to over come compilation and linking issues faced during porting my C++ application from RHEL.BackgroundI was working on porting an C++ Application from RHEL to Solaris 10. I was getting too many compile time errors when recompiling the application on Solaris. Initially I tried fixing those errors without thinking that language also has progressed and we might need a new compiler. After a few days finally I started comparing the build environment and closed on compiler version being different.Upgrading CompilerInitially like anybody else, we took the latest version of gcc available on Solaris 10 and tried compiling my code. It never worked out. I realized that language standard have progressed much further and gcc use to throw new errors which would have taken long time for me to fix.pkg install gcc-45Compiling the CompilerAfter my failed attempt to compile my code using latest gcc compiler, I started searching for pre-built packages for gcc 4.1.2 on Solaris 10 without any luck.Finally we made a decision to compile the compiler ourself. I immediately downloaded gcc 4.1.2 from gcc’s website. I saw that there is a configure script and Makefile. Looking at the documentation I ran configure script with bare minimum options followed by gmake. The compiler worked and I was able to compile my code. The testing cycle could start now.pkg install gcc-3cd gcc_source_dirmkdir objdir &amp;&amp; cd objdir../configure --enable-languages=c,c++gmakegmake installNext day, while reviewing my colleague’s code I found that there is a security issue with the compiler which we compiled. All the private methods were accessible in our library and anybody could call those methods.nm /path/to/library.a...00000000000021b0 T CLIF_arg_double00000000000021e0 T CLIF_arg_func0000000000002190 T CLIF_arg_int0000000000002140 T CLIF_arg_string00000000000021a0 T CLIF_arg_uint00000000000021c0 T CLIF_call_func0000000000001600 T CLIF_current_help0000000000001670 T CLIF_parse0000000000000fb0 T CLIF_print_arguments0000000000000e00 T CLIF_print_options0000000000001100 T CLIF_print_usage0000000000002180 T CLIF_set_double00000000000020e0 T CLIF_set_flag0000000000002160 T CLIF_set_int0000000000002120 T CLIF_set_string0000000000002170 T CLIF_set_uint0000000000002100 T CLIF_unset_flag00000000000020a0 T CLIF_version_handler...Upon debugging the library, I found that all the public/private functions which we wrote had a T denoting that the functions are public. We again started compiling gcc with different arguments. After many attempts there was a time when all of the functions even the public ones became private.Now many iterations later we were able to compile gcc for our use case using following steps: Install gcc 3.4.3pkg install gcc-3 By default Solaris 10 comes with gcc 3.4.3 and binutils 2.15. We need to compile binutils 2.18 in order gcc compiled correctly. Prepare the shell for compiling gcc:# Use ksh shellexport CONFIG_SHELL=/usr/bin/ksh# Copy/Create links to all gnu tools into one directory and remember to remove letter 'g' from the prefix of the name of executable.export PATH=/Mytools/binutils_2_18/bin:/gnutools:/sbin:/bin:/usr/bin:/usr/ccs/bin:/usr/sfw/binexport CC=\"/usr/sfw/bin/gcc -fPIC\"export CXX=\"/usr/sfw/bin/g++ -fPIC\" /Mytools/binutils_2_18 is the location where binutils 2.18 is installed. /gnutools is the location where gnu tools like gmake, gawk, gsed and gmake etc are symlinked without the g prefix. Build:cd gcc_source_dirmkdir objdir &amp;&amp; cd objdir../configure --with-gnu-as --with-as=/binutils_2_18/bin/as --with-gnu-ld --with-ld=/binutils_2_18/bin/ld --prefix=$PREFIX --enable-threads=posix --enable-checking=release --with-system-zlib --enable-shared --disable-symvers --enable-languages=c,c++gmake #referring to gnu makegmake install $PREFIX is the location where you want to install gcc. It should be writable by your user.LearningsI learned that compiling a compiler without a compiler is kind of fun. Following are the things I learned: gcc built a basic compiler before actually compiling the code. gcc configure script has so many flags which can cause security issues in the output binary like the one time when I acciedently made all the functions public." } ]
